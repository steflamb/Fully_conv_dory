{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "03402649",
   "metadata": {},
   "source": [
    "# A proposal of a unified quantization specification\n",
    "\n",
    "The purpose of this document is analysing the problem of **quantizers** and deriving a unified specification to describe them.\n",
    "Although we will start with a mathematical formalisation, our goal is to derive practical descriptions that might be useful in practical engineering contexts.\n",
    "\n",
    "\n",
    "## Terminology\n",
    "\n",
    "### Quantization levels, jumps\n",
    "\n",
    "Let $K > 1$ be a positive integer, called the *number of quantization levels*, or simply the **number of levels**.\n",
    "Given $K$, we define the **quantization levels** as the elements of the set\n",
    "\n",
    "\\begin{equation*}\n",
    "    Q := \\{ q_{0} < \\dots < q_{K-1} \\} \\subset \\mathbb{R} \\,.\n",
    "\\end{equation*}\n",
    "\n",
    "Note that the set is finite, and that its elements are *ordered*.\n",
    "We call this set a **quantization set** of cardinality $K$.\n",
    "\n",
    "Given a quantization set $Q$ of cardinality $K$, for each $k = 1, \\dots, K-1$ we define the quantity\n",
    "\n",
    "\\begin{equation*}\n",
    "    j_{k} := q_{k} - q_{k-1}\n",
    "\\end{equation*}\n",
    "\n",
    "and call it the **jump** between the $(k-1)$-th and the $k$-th quantization levels.\n",
    "Note that, by definition of quantization set, $j_{k} > 0, \\,\\forall\\, k = 1, \\dots, K-1$.\n",
    "We define the **jumps set** as\n",
    "\n",
    "\\begin{equation*}\n",
    "    J := \\{ j_{1}, \\dots, j_{K-1} \\} \\,.\n",
    "\\end{equation*}\n",
    "\n",
    "Note that this set is not necessarily ordered, nor the jumps are required to be mutually equal.\n",
    "\n",
    "\n",
    "### Binning thresholds, gaps\n",
    "\n",
    "Let $K > 1$ be a number of quantization levels.\n",
    "We define the **binning thresholds**, or simply, *thresholds*, to be the elements of the set\n",
    "\n",
    "\\begin{equation*}\n",
    "    T := \\{ \\theta_{1} < \\dots < \\theta_{K-1} \\} \\subset \\mathbb{R} \\,.\n",
    "\\end{equation*}\n",
    "\n",
    "Note that also this set is finite, and that its elements are also *ordered*.\n",
    "We call this set a **binning thresholds set**, or simply *thresholds set*, of cardinality $K-1$.\n",
    "\n",
    "Similarly to the definition of jumps, given a thresholds set $T$ of cardinality $K-1$, for each $k = 2, \\dots, K-1$ we define the quantity\n",
    "\n",
    "\\begin{equation*}\n",
    "    g_{k} := \\theta_{k} - \\theta_{k-1}\n",
    "\\end{equation*}\n",
    "\n",
    "and call it the **gap** between the $(k-1)$-th and the $k$-th thresholds.\n",
    "Note that, by definition of thresholds set, $g_{k} > 0, \\,\\forall\\, k = 2, \\dots, K-1$.\n",
    "We define the **gaps set** as\n",
    "\n",
    "\\begin{equation*}\n",
    "    G := \\{ g_{2}, \\dots, g_{K-1} \\} \\,.\n",
    "\\end{equation*}\n",
    "\n",
    "Note that this set is not necessarily ordered, nor the gaps are required to be mutually equal.\n",
    "\n",
    "\n",
    "## Binning $\\mathbb{R}$\n",
    "\n",
    "Let $K > 1$ be a number of quantization levels, $Q$ be a quantization set of cardinality $K$, $T$ be a thresholds set of cardinality $K-1$.\n",
    "\n",
    "Due to the ordering property of $T$, the thresholds naturally define a partition of $\\mathbb{R}$ into $2K - 1$ subsets.\n",
    "If (with an abuse of mathematical notation) we define $\\theta_{0} := \\inf \\mathbb{R} = -\\infty$ and $\\theta_{K} := \\sup \\mathbb{R} = +\\infty$, for each $k = 0, \\dots, K-1$ we can define the open interval\n",
    "\n",
    "\\begin{equation*}\n",
    "    \\tilde{I}_{k} := (\\theta_{k}, \\theta_{k+1}) \\,.\n",
    "\\end{equation*}\n",
    "\n",
    "Moreover, for each $k = 1, \\dots, K-1$ let us define $\\Theta_{k} = \\{ \\theta_{k} \\}$, i.e., the *singletons* including just the threshold point each.\n",
    "\n",
    "With this notation, it is straightforward to represent the partition of $\\mathbb{R}$ as\n",
    "\n",
    "\\begin{equation*}\n",
    "    \\tilde{P}_{T} = \\{ \\tilde{I}_{0}, \\Theta_{1}, \\tilde{I}_{1}, \\dots, \\tilde{I}_{K-2}, \\Theta_{K-1}, \\tilde{I}_{K-1} \\} \\,.\n",
    "\\end{equation*}\n",
    "\n",
    "Intuitively, each element of $\\tilde{P}$ represents a \"bin\" to which a real value $x \\in \\mathbb{R}$ can be mapped.\n",
    "\n",
    "In the semantics of (quantized) deep neural networks we consider the elements $x \\in \\mathbb{R}$ as \"stimuli\", and for each stimulus we would like to associate a \"response\" chosen from the set $Q$.\n",
    "An intuitive consequence of this semantics is that we want to label stimuli monotonically; i.e., given two stimuli $x_{1}, x_{2} \\in \\mathbb{R}, x_{1} < x_{2}$ we would like that the responses satisfy $\\sigma(x_{1}) \\leq \\sigma(x_{2})$.\n",
    "\n",
    "Consequently, we want to label each element in $\\tilde{P}$ with an element of the quantization set $Q$.\n",
    "Since a constant response does not make an interesting system component, we want to use all the elements in $Q$ (remember that $|Q| = K > 1$).\n",
    "Since we have $K$ quantization levels and $K$ non-singleton intervals, we will assign the response $q_{k}$ to all the stimuli in bin $\\tilde{I}_{k}$, for each $k = 0, \\dots, K-1$.\n",
    "\n",
    "Multiple choices can be made for what concerns labelling stimuli $x \\in T$ (i.e., inputs that fall precisely on a threshold).\n",
    "Each threshold is placed exactly in-between two consecutive bins.\n",
    "Indeed, given $k \\in \\{ 1, ..., K-1 \\}$, by definition of $\\tilde{I}_{k}$ it follows that $\\forall\\, x \\in I_{k-1}, \\, x < \\theta_{k}$; analogously, $\\forall\\, x \\in I_{k}, \\, \\theta_{k} < x$.\n",
    "Due to the monotonicity requirement, we can map $\\theta_{k}$ either to $q_{k-1}$ or to $q_{k}$.\n",
    "This reasoning is valid for each threshold, and we therefore have $2^{K-1}$ options to map thresholds into $Q$ in an way that satisfies the monotonicity constraint.\n",
    "We **must** choose a convention.\n",
    "\n",
    "An example convention might be to map\n",
    "\n",
    "\\begin{equation*}\n",
    "    \\theta_{k} \\mapsto \\max \\{ q_{k-1}, q_{k} \\} = q_{k} \\,.\n",
    "\\end{equation*}\n",
    "\n",
    "This will yield the partition\n",
    "\n",
    "\\begin{equation*}\n",
    "    P_{T}^{\\geq} := \\{ I_{0} = \\tilde{I}_{0} \\} \\cup ( \\cup_{k = 1}^{K-1} (I_{k} := \\tilde{I}_{k} \\cup \\Theta_{k})) \\,.\n",
    "\\end{equation*}\n",
    "\n",
    "We call this partition the **greater-or-equal partition**.\n",
    "\n",
    "\n",
    "### Quantizers\n",
    "\n",
    "We define a **proto-quantizer** to be a function $\\sigma$ that satisfies the following properties:\n",
    "\n",
    "* its domain is $\\mathbb{R}$ or a subset of its; i.e., $\\mathcal{D}_{\\sigma} \\subseteq \\mathbb{R}$;\n",
    "* its codomain is a (finite) quantization set $Q \\subset \\mathbb{R}$; i.e., $\\mathcal{C}_{\\sigma} = Q \\subset \\mathbb{R}$;\n",
    "* it is non-decreasing; i.e., $\\forall\\, x_{1}, x_{2} \\in \\mathcal{D}_{\\sigma}, x_{1} < x_{2}, \\, \\sigma(x_{1}) \\leq \\sigma(x_{2})$;\n",
    "* it is non-constant; i.e., $\\exists\\, x_{1}, x_{2} \\in \\mathcal{D}_{\\sigma}, x_{1} \\neq x_{2} \\,|\\, \\sigma(x_{1}) \\neq \\sigma(x_{2})$.\n",
    "\n",
    "The simplest example of proto-quantizer is the **Heaviside function**:\n",
    "\n",
    "\\begin{align*}\n",
    "    H \\,:\\,\n",
    "    &\\mathbb{R} \\to \\{ 0, \\frac{1}{2}, 1 \\} \\\\\n",
    "    &x \\mapsto\n",
    "    \\begin{cases}\n",
    "        0, \\text{if } x < 0, \\\\\n",
    "        \\frac{1}{2}, \\text{if } x = 0, \\\\\n",
    "        1, \\text{if } x > 0.\n",
    "    \\end{cases}\n",
    "\\end{align*}\n",
    "\n",
    "Alternative versions of the Heaviside map zero to either zero or one, simplifying the function:\n",
    "\n",
    "\\begin{align*}\n",
    "    H^{-} \\,:\\,\n",
    "    &\\mathbb{R} \\to \\{ 0, 1 \\} \\\\\n",
    "    &x \\mapsto\n",
    "    \\begin{cases}\n",
    "        0, \\text{if } x \\leq 0, \\\\\n",
    "        1, \\text{if } x > 0,\n",
    "    \\end{cases} \\\\\n",
    "    H^{+} \\,:\\,\n",
    "    &\\mathbb{R} \\to \\{ 0, 1 \\} \\\\\n",
    "    &x \\mapsto\n",
    "    \\begin{cases}\n",
    "        0, \\text{if } x < 0, \\\\\n",
    "        1, \\text{if } x \\geq 0.\n",
    "    \\end{cases}\n",
    "\\end{align*}\n",
    "\n",
    "In digital arithmetic, $H^{-}$ and $H^{+}$ serve as better models that $H$, since they can be implemented using a single comparison instruction instead of two.\n",
    "\n",
    "Given a parameter $\\theta \\in \\mathbb{R}$, we define the **generalised Heaviside function** as\n",
    "\n",
    "\\begin{align*}\n",
    "    H_{\\theta} \\,:\\,\n",
    "    &\\mathbb{R} \\to \\{ 0, \\frac{1}{2}, 1 \\} \\\\\n",
    "    &x \\mapsto H(x - \\theta) =\n",
    "    \\begin{cases}\n",
    "        0, \\text{if } x < \\theta, \\\\\n",
    "        \\frac{1}{2}, \\text{if } x = \\theta, \\\\\n",
    "        1, \\text{if } x > \\theta.\n",
    "    \\end{cases}\n",
    "\\end{align*}\n",
    "\n",
    "Analogously, we define $H^{-}_{\\theta}(x) := H^{-}(x - \\theta)$ and $H^{+}_{\\theta}(x) := H^{+}(x - \\theta)$ for all $x \\in \\mathbb{R}$.\n",
    "\n",
    "Let now $K > 1$ be a given number of quantization levels, $Q$ be a quantization set of cardinality $K$, and $T$ be a thresholds set of cardinality $K-1$.\n",
    "We define a **quantizer** as the function\n",
    "\n",
    "\\begin{align*}\n",
    "\\begin{split}\n",
    "    \\sigma \\,:\\,\n",
    "    &\\mathbb{R} \\to Q \\\\\n",
    "    &x \\mapsto q_{0} + \\sum_{k = 1}^{K-1} j_{k} H^{+}_{\\theta}(x) \\,.\n",
    "\\end{split}\n",
    "\\end{align*}\n",
    "\n",
    "We could also express this quantizer as the weighted sum of the indicator functions associated with the greater-or-equal partition $P_{T}^{\\geq} = \\{ I_{0}, \\dots, I_{K-1} \\}$:\n",
    "\n",
    "\\begin{align*}\n",
    "\\begin{split}\n",
    "    \\sigma \\,:\\,\n",
    "    &\\mathbb{R} \\to Q \\\\\n",
    "    &x \\mapsto \\sum_{k = 0}^{K-1} q_{k} \\chi_{I_{k}}(x) \\,.\n",
    "\\end{split}\n",
    "\\end{align*}\n",
    "\n",
    "When $\\mathcal{D}_{\\sigma} = \\mathbb{R}$, the graph of a quantizer resembles the shape of a stair, and we hence call them also *stair functions*.\n",
    "In the specific sub-case where $|Q| = 2$, the graph of a quantizer resembles the shape of a stair composed of a single step, and we hence call it a *step function*.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73a62298",
   "metadata": {},
   "source": [
    "## Constraining the domain\n",
    "\n",
    "In digital arithmetic applications, the domain of a quantizer can not be the whole set $\\mathbb{R}$.\n",
    "In practice, $\\mathcal{D}_{\\sigma}$ is a finite subset of $\\mathbb{Q} \\subset \\mathbb{R}$ (in the case of floating-point of fixed-point decimal numbers), a finite subset of $\\mathbb{Z} \\subset \\mathbb{Q} \\subset \\mathbb{R}$ (in the case of signed integers), or a finite subset of $\\mathbb{N}_{0} \\subset \\mathbb{Z} \\subset \\mathbb{Q} \\subset \\mathbb{R}$ (in the case of unsigned integers).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d50a3e10",
   "metadata": {},
   "source": [
    "## Decomposing a quantizer\n",
    "\n",
    "We argue that each quantizer function can be decomposed into three operations:\n",
    "\n",
    "* **binning**, where the input range is mapped into an integer range (representing abstract bins; we could as well represent this range using a categorical variable);\n",
    "* **re-indexing**, where an integer range (representing abstract bins) is mapped to another integer range (representing numbers that can be used by hardware logic, such as integer multipliers or bit-shift values); an example of a case where it's needed is provided by the case of asymmetric uniform linear quantization, where we want to represent an \"uncommon\" range of bins (e.g., -27 to 123);\n",
    "* **de-integerization**, where an integer range (representing numbers that can be used by hardware logic) is mapped to the output range; such mapping can be the identity (this is the case of true-quantized networks) or involve some embedding into the floating-point domain (e.g., when multiplying the integer value by the *output quantum* or expanding a bit-shift into a power-of-two multiplier).\n",
    "\n",
    "From the hardware perspective, some or all of these operations are usually handled implicitely, so this might seem as an hopeless or unuseful (and hence unnecessary) task.\n",
    "But remember that the goal of this document is looking for a general specification that can simplify graph rewriting and compilation.\n",
    "\n",
    "*Therefore, we will try to define a way in which all hardware quantisation operations can be mapped back to this formalisation, and viceversa.*\n",
    "\n",
    "\n",
    "### Considerations\n",
    "\n",
    "Many algorithms perform the binning step on parameters at training time, but note that it is **not** mandatory to perform it at inference time.\n",
    "In fact, since parameters won't change once the network is deployed, this step should be executed only once (after training, during the code generation step).\n",
    "\n",
    "Instead, the binning step must **always** be performed on features at runtime.\n",
    "In fact, since features can change once the network is deployed, this step will have to be executed once per inference.\n",
    "\n",
    "Performing the binning in a threshold-based manner is very general but usually inefficient from an application perspective.\n",
    "\n",
    "Most algorithms proposed in the literature tackle this problem by an efficient use of hardware-friendly operations such as:\n",
    "\n",
    "* flooring/ceiling/rounding (on floating-point numbers) and bit-shifts (on integer numbers) to truncate the fractional parts of numbers,\n",
    "* clipping to limit the domain and/or the codomain of a quantizer (recall that this operation is based on the max/min operations, which are ultimately based on comparisons).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a2bc6fb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from typing import Union, Tuple\n",
    "\n",
    "\n",
    "# generate input array\n",
    "xmin = -5.0\n",
    "xmax = 8.0\n",
    "N    = 10000\n",
    "x    = np.linspace(xmin, xmax, N)\n",
    "\n",
    "\n",
    "def show_quantization(\n",
    "    x:            np.array, \n",
    "    x_binned:     Union[np.array, None],\n",
    "    x_int:        Union[np.array, None],\n",
    "    z:            np.array,\n",
    "    fraction:     float = 0.1,\n",
    "    random_start: bool = False):\n",
    "\n",
    "    period = np.floor(len(x) * fraction).astype(dtype=np.int64)  # show just one item over `period`\n",
    "\n",
    "    if random_start:\n",
    "        s = np.random.randint(0, period)\n",
    "    else:\n",
    "        s = 0\n",
    "\n",
    "    e = len(x)\n",
    "    \n",
    "    str_out = \"Original: {:8.3f} - Bin ID: {:4d} - Encoding: {:4d} - De-integerised: {:8.3f}\"\n",
    "    for i in range(s, e, period):\n",
    "        print(str_out.format(x[i], x_binned[i], x_int[i].astype(np.int64), z[i]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "841b8580",
   "metadata": {},
   "source": [
    "## Threshold-based quantization\n",
    "\n",
    "Originally, the *additive noise annealing* (ANA) quantization algorithm was meant to use uniform output ranges (i.e., the jumps between consecutive quantization levels are mutually equal) and possibly non-uniformly-spaced binning thresholds (i.e., the gaps between two consecutive thresholds could be distinct).\n",
    "In this case, the binning step must be performed using a look-up table (i.e., comparison-based) approach.\n",
    "\n",
    "We can refer to this type of quantization algorithms as *non-uniform binning, uniform de-integerisation*.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9ffade0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlevels = 8\n",
    "\n",
    "\n",
    "def generate_thresholds(\n",
    "    nlevels: int,\n",
    "    tmin:    float = -1.0,\n",
    "    tmax:    float = 1.0) -> np.array:\n",
    "\n",
    "    assert tmin < tmax\n",
    "\n",
    "    nthres = nlevels - 1\n",
    "    thres  = (tmax - tmin) * np.random.rand(nlevels - 1) + tmin\n",
    "    thres  = np.sort(thres)\n",
    "    \n",
    "    return thres\n",
    "\n",
    "\n",
    "# thresholds and binning\n",
    "thres     = generate_thresholds(nlevels, tmin=-3.0, tmax=3.0)\n",
    "thres_ext = np.concatenate((np.array([-np.inf]), thres, np.array([np.inf])), axis=0)\n",
    "\n",
    "bin_preimages = [(s, e) for s, e in zip(thres_ext[:-1], thres_ext[1:])]\n",
    "bin_ids       = np.arange(0, len(bin_preimages))\n",
    "\n",
    "bin2preimage = {bin_id: preimage for bin_id, preimage in zip(bin_ids, bin_preimages)}\n",
    "bin2thres    = {bin_id: t for bin_id, t in zip(bin_ids[1:], thres)}\n",
    "\n",
    "\n",
    "# re-indexing\n",
    "zero      = -4\n",
    "encodings = np.arange(zero, zero + nlevels)\n",
    "\n",
    "bin2enc = {bin_id: enc for bin_id, enc in zip(bin_ids, encodings)}\n",
    "\n",
    "\n",
    "# de-integerisation\n",
    "quantum_out = 0.1\n",
    "offset_out  = 2.3\n",
    "\n",
    "\n",
    "def binning(\n",
    "    x:            np.array,\n",
    "    bin2preimage: dict,\n",
    "    bin2thres:    dict) -> np.array:\n",
    "    \n",
    "    x_binned = np.zeros_like(x, dtype=np.int64)\n",
    "    \n",
    "    for bin_id, (s, e) in bin2preimage.items():\n",
    "        x_in_bin = np.logical_and(s < x, x < e)\n",
    "        x_binned[x_in_bin] = bin_id\n",
    "        \n",
    "    for bin_id, t in bin2thres.items():\n",
    "        x_binned[x == t] = bin_id\n",
    "        \n",
    "    return x_binned\n",
    "\n",
    "\n",
    "def reindexing(\n",
    "    x_binned: np.array,\n",
    "    bin2enc:  dict) -> np.array:\n",
    "    \n",
    "    x_int = np.array(list(map(lambda x: bin2enc[x], x_binned)))\n",
    "    \n",
    "    return x_int\n",
    "\n",
    "\n",
    "def deintegerisation(\n",
    "    x_int:       np.array,\n",
    "    quantum_out: float,\n",
    "    offset_out:  float) -> np.array:\n",
    "    \n",
    "    return x_int * quantum_out + offset_out\n",
    "\n",
    "\n",
    "x_binned = binning(x, bin2preimage, bin2thres)\n",
    "x_int    = reindexing(x_binned, bin2enc)\n",
    "z        = deintegerisation(x_int, quantum_out, offset_out)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e64c442",
   "metadata": {},
   "source": [
    "For performance reasons, the binning step can fused with the re-indexing step.\n",
    "This can be easily implemented by storing the integer encoding value (which as an arithmetic semantic) into the look-up table, without the need of an extra indirection level.\n",
    "\n",
    "This is an implicit but ubiquitous practice in most quantization algorithms.\n",
    "It derives from the fact that in most quantization algorithms the bins are consecutive intervals defining a partition of $\\mathbb{R}$.\n",
    "Therefore, it is possible to number them using a sequence of successive integers.\n",
    "Moreover, since most algorithms assume that the quantization levels are associated to bins in a monotonic fashion, the re-indexing operation can be somehow straightforwardly composed with the binning.\n",
    "\n",
    "Note, though, that this assumption might not hold for more general quantizers!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "359dbe40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build look-up table\n",
    "thres2enc = {t: bin2enc[bin_id] for bin_id, t in bin2thres.items()}\n",
    "\n",
    "\n",
    "def fused_binning_reindexing(\n",
    "    x:         np.array,\n",
    "    enc_0:     int,\n",
    "    thres2enc: dict) -> np.array:\n",
    "    \n",
    "    # I assume that the integer encodings are perfectly correlated with thresholds;\n",
    "    # i.e., if (t1, e1) and (t2, e2) are items of the LUT, then t1 < t2 -> e1 < e2\n",
    "    \n",
    "    x_int = np.full(x.shape, enc_0)\n",
    "    \n",
    "    for t, enc in thres2enc.items():\n",
    "        x_int[t <= x] = enc\n",
    "    \n",
    "    return x_int\n",
    "\n",
    "\n",
    "fused_x_int = fused_binning_reindexing(x, encodings[0], thres2enc)\n",
    "fused_z     = deintegerisation(fused_x_int, quantum_out, offset_out)\n",
    "\n",
    "assert np.all(fused_x_int == x_int)\n",
    "assert np.all(fused_z     == z)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17c3bf3e",
   "metadata": {},
   "source": [
    "## Logarithmic quantization\n",
    "\n",
    "As an example quantization algorithm using logarithmic binning and logarithmic de-integerisation, we consider the *incremental network quantization* (INQ) algorithm.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ae44b7db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# binning parameters\n",
    "offset_in  = 0.0\n",
    "stretch_in = 1.0\n",
    "base_in    = 2.0\n",
    "\n",
    "nmin = -2\n",
    "nmax = 2\n",
    "pmin = -3\n",
    "pmax = 3\n",
    "\n",
    "\n",
    "# thresholds and binning\n",
    "nrange = np.arange(nmin, nmax + 1)\n",
    "nthres = offset_in - ((base_in + 1) / 2) * stretch_in * (np.exp2(nrange - 1) ** np.log2(base_in))\n",
    "nthres = np.sort(nthres)  # sort thresholds in ascending order\n",
    "\n",
    "prange = np.arange(pmin, pmax + 1)\n",
    "pthres = offset_in + ((base_in + 1) / 2) * stretch_in * (np.exp2(prange - 1) ** np.log2(base_in))\n",
    "pthres = np.sort(pthres)  # sort thresholds in ascending order\n",
    "\n",
    "thres     = np.concatenate((nthres, pthres), axis=0)\n",
    "thres_exp = np.concatenate((np.array([-np.inf]), thres, np.array([np.inf])), axis=0)\n",
    "\n",
    "bin_preimages = [(s, e) for s, e in zip(thres_exp[:-1], thres_exp[1:])]\n",
    "bin_ids       = np.arange(0, len(bin_preimages))  # abstract categorical identifiers (I use integers, but I could as well use strings from a generic alphabet)\n",
    "bin2preimage  = {bin_id: preimage for bin_id, preimage in zip(bin_ids, bin_preimages)}\n",
    "\n",
    "bin2nthres    = {bin_id: t for bin_id, t in zip(bin_ids[:len(nthres)],  nthres)}\n",
    "bin2pthres    = {bin_id: t for bin_id, t in zip(bin_ids[-len(pthres):], pthres)}\n",
    "thres2bin     = {**bin2nthres, **bin2pthres}\n",
    "\n",
    "\n",
    "# re-indexing\n",
    "sign      = np.concatenate((-np.ones(len(nrange)), np.zeros(1), np.ones(len(prange))), axis=0)\n",
    "shift     = np.concatenate((nrange[::-1], np.array([-np.inf]), prange), axis=0)\n",
    "encodings = list(zip(sign, shift))\n",
    "\n",
    "bin2enc = {b: (sg, sh) for b, (sg, sh) in zip(bin_ids, encodings)}\n",
    "\n",
    "\n",
    "# de-integerisation parameters\n",
    "base_out    = 2.0\n",
    "stretch_out = 1.0\n",
    "offset_out  = 0.0\n",
    "\n",
    "# quantization levels\n",
    "nqlevels = offset_out - stretch_out * (np.exp2(nrange) ** np.log2(base_out))\n",
    "pqlevels = offset_out + stretch_out * (np.exp2(prange) ** np.log2(base_out))\n",
    "\n",
    "qlevels = np.concatenate((nqlevels, np.zeros(1), pqlevels), axis=0)\n",
    "qlevels = np.sort(qlevels)\n",
    "\n",
    "def binning(\n",
    "    x:            np.array,\n",
    "    bin2preimage: dict,\n",
    "    bin2thres:    dict) -> np.array:\n",
    "\n",
    "    x_binned = np.zeros_like(x, dtype=np.int64)\n",
    "    \n",
    "    for bin_id, (s, e) in bin2preimage.items():\n",
    "        x_in_bin = np.logical_and(s < x, x < e)\n",
    "        x_binned[x_in_bin] = bin_id\n",
    "    \n",
    "    for bin_id, t in bin2thres.items():\n",
    "        x_binned[x == t] = bin_id\n",
    "        \n",
    "    return x_binned\n",
    "\n",
    "\n",
    "def reindexing(\n",
    "    x_binned: np.array,\n",
    "    bin2enc:  dict) -> Tuple[np.array, np.array]:\n",
    "    \n",
    "    sign  = np.array(list(map(lambda x: bin2enc[x][0], x_binned)))\n",
    "    shift = np.array(list(map(lambda x: bin2enc[x][1], x_binned)))\n",
    "    \n",
    "    return sign, shift\n",
    "\n",
    "\n",
    "def deintegerization(\n",
    "    sign:        np.array,\n",
    "    shift:       np.array,\n",
    "    base_out:    float,\n",
    "    stretch_out: float,\n",
    "    offset_out:  float) -> np.array:\n",
    "    \n",
    "    level = sign * np.exp2(shift) ** np.log2(base_out)\n",
    "    \n",
    "    return level * stretch_out + offset_out\n",
    "\n",
    "\n",
    "x_binned    = binning(x, bin2preimage, bin2thres)\n",
    "sign, shift = reindexing(x_binned, bin2enc)\n",
    "z           = deintegerization(sign, shift, base_out, stretch_out, offset_out)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0717ca0",
   "metadata": {},
   "source": [
    "For efficiency reasons, the binning and re-indexing steps are fused together.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b93c1aae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fused_binning_reindexing(\n",
    "    x:          np.array,\n",
    "    offset_in:  float,\n",
    "    stretch_in: float,\n",
    "    base_in:    float,\n",
    "    nmin:       int,\n",
    "    nmax:       int,\n",
    "    pmin:       int,\n",
    "    pmax:       int) -> Tuple[np.array, np.array]:\n",
    "    \n",
    "    assert 0 < stretch_in  # scale factor must be positive\n",
    "    assert 0 < base_in     # representation base must be positive\n",
    "    \n",
    "    assert nmin < nmax\n",
    "    assert pmin < pmax\n",
    "    \n",
    "    x_centered = x - offset_in\n",
    "    x_scaled   = x_centered / stretch_in\n",
    "\n",
    "    sign = np.sign(x_centered)\n",
    "    \n",
    "    factor = (base_in + 1) / 2\n",
    "    shift  = np.floor(np.log2(x_scaled * sign / factor) / np.log2(base_in)) + 1.0\n",
    "    \n",
    "    shift_negative = shift[sign < 0]\n",
    "    shift_positive = shift[sign >= 0]\n",
    "    \n",
    "    shift[sign < 0]  = np.where(shift_negative < nmin, np.full(shift_negative.shape, -np.inf), np.clip(shift_negative, nmin, nmax))\n",
    "    shift[sign >= 0] = np.where(shift_positive < pmin, np.full(shift_positive.shape, -np.inf), np.clip(shift_positive, pmin, pmax))\n",
    "    \n",
    "    sign[shift == -np.inf] = 0.0\n",
    "    \n",
    "    return sign, shift\n",
    "\n",
    "\n",
    "fused_sign, fused_shift = fused_binning_reindexing(x, offset_in, stretch_in, base_in, nmin, nmax, pmin, pmax)\n",
    "fused_z                 = deintegerization(sign, shift, base_out, stretch_out, offset_out)\n",
    "\n",
    "\n",
    "assert np.all(fused_sign  == sign)\n",
    "assert np.all(fused_shift == shift)\n",
    "assert np.all(fused_z     == z)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3808308",
   "metadata": {},
   "source": [
    "We should also specify that the INQ algorithm does not leave as many degree of freedoms as we did in the code above.\n",
    "The specific assumptions of the algorithm are the following:\n",
    "\n",
    "* $\\mathtt{nmin} = \\mathtt{pmin}$;\n",
    "* $\\mathtt{nmax} = \\mathtt{pmax}$;\n",
    "* the total number of levels, $(\\mathtt{pmax} - \\mathtt{pmin} + 1) + (\\mathtt{pmax} - \\mathtt{pmin} + 1) + 1$ is equal to $2^{B} - 1$ for some $B \\in \\mathbb{N}, B > 1$;\n",
    "* $\\mathtt{offset\\_in} = \\mathtt{offset\\_out} = 0$;\n",
    "* $\\mathtt{stretch\\_in} = \\mathtt{stretch\\_out} = 0$;\n",
    "* $\\mathtt{base\\_in} = \\mathtt{base\\_out} = 2$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8821a82a",
   "metadata": {},
   "source": [
    "## Uniform quantization (AKA linear quantization)\n",
    "\n",
    "Linear binning and linear de-integerisation are used by many quantization algorithms.\n",
    "Example algorithms include the *straight-through estimator* (STE), *parametrized activation clipping* (PACT), and *statistics-aware weight binning* (SAWB).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "45e5f9fa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# quantization parameters (word-size specification)\n",
    "use_wordsize_nlevels = False\n",
    "\n",
    "if use_wordsize_nlevels:\n",
    "    nbits   = 8\n",
    "    nlevels = 2 ** B\n",
    "else:\n",
    "    nbits_max = 8\n",
    "    nlevels   = np.random.randint(2, 2 ** nbits_max + 1)  # in applications, the designer would choose how many levels to use\n",
    "\n",
    "\n",
    "# the bins are computed on tensor statistics\n",
    "use_symmetric_encoding = False\n",
    "\n",
    "if use_symmetric_encoding:\n",
    "\n",
    "    span = 2 * np.max(np.abs(x))\n",
    "\n",
    "    quantum_in = span / (nlevels - 1) if (nlevels % 2 == 1) else span / (nlevels - 2)\n",
    "    zero       = - (nlevels - 1) / 2 if (nlevels % 2 == 1) else - ((nlevels - 2) / 2 + 1)\n",
    "\n",
    "else:\n",
    "\n",
    "    xmin = np.min(x)\n",
    "    xmax = np.max(x)\n",
    "    span = np.max(x) - np.min(x)\n",
    "\n",
    "    quantum_in = span / (nlevels - 1)\n",
    "    zero      = np.floor(xmin / quantum_in)\n",
    "\n",
    "# thresholds and binning\n",
    "thres     = quantum_in * (zero + np.arange(1, nlevels))\n",
    "thres     = np.sort(thres)\n",
    "thres_ext = np.concatenate((np.array([-np.inf]), thres, np.array([np.inf])), axis=0)\n",
    "\n",
    "bin_preimages = [(s, e) for s, e in zip(thres_ext[:-1], thres_ext[1:])]\n",
    "bin_ids       = np.arange(0, len(bin_preimages))\n",
    "bin2preimage  = {bin_id: preimage for bin_id, preimage in zip(bin_ids, bin_preimages)}\n",
    "bin2thres     = {bin_id: t for bin_id, t in zip(bin_ids[1:], thres)}\n",
    "\n",
    "\n",
    "# re-indexing\n",
    "encodings = np.array([bin_id + zero for bin_id in bin_ids])\n",
    "bin2enc   = {bin_id: enc for bin_id, enc in zip(bin_ids, encodings)}\n",
    "\n",
    "\n",
    "# de-integerisation parameters\n",
    "quantum_out = quantum_in\n",
    "qlevels     = encodings * quantum_out\n",
    "\n",
    "\n",
    "def binning(\n",
    "    x:            np.array,\n",
    "    bin2preimage: dict,\n",
    "    bin2thres:    dict) -> np.array:\n",
    "    \n",
    "    x_binned = np.zeros_like(x, dtype=np.int64)\n",
    "    \n",
    "    for bin_id, (s, e) in bin2preimage.items():\n",
    "        x_in_bin = np.logical_and(s < x, x < e)\n",
    "        x_binned[x_in_bin] = bin_id\n",
    "    \n",
    "    for bin_id, t in bin2thres.items():\n",
    "        x_binned[x == t] = bin_id\n",
    "        \n",
    "    return x_binned\n",
    "\n",
    "\n",
    "def reindexing(\n",
    "    x_binned: np.array,\n",
    "    bin2enc:  dict) -> np.array:\n",
    "    \n",
    "    x_int = np.array(list(map(lambda x: bin2enc[x], x_binned)))\n",
    "\n",
    "    return x_int\n",
    "    \n",
    "\n",
    "def deintegerisation(\n",
    "    x_int:       np.array,\n",
    "    quantum_out: float,\n",
    "    offset_out:  float) -> np.array:\n",
    "    \n",
    "    return x_int * quantum_out + offset_out\n",
    "\n",
    "\n",
    "x_binned = binning(x, bin2preimage, thres2bin)\n",
    "x_int    = reindexing(x_binned, bin2enc)\n",
    "z        = deintegerisation(x_int, quantum_out, offset_out)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe61d96c",
   "metadata": {},
   "source": [
    "Again, for performance reason the binning and re-indexing operations are usually fused.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "04c791d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fused_binning_reindexing(\n",
    "    x:          np.array,\n",
    "    quantum_in: float,\n",
    "    zero:       int,\n",
    "    nlevels:    int) -> np.array:\n",
    "    \n",
    "    x_int = np.clip(np.floor(x / quantum_in), zero, zero + nlevels - 1)\n",
    "    \n",
    "    return x_int\n",
    "\n",
    "fused_x_int = fused_binning_reindexing(x, quantum_in, zero, nlevels)\n",
    "fused_z     = deintegerisation(fused_x_int, quantum_out, offset_out)\n",
    "\n",
    "assert np.all(fused_x_int == x_int)\n",
    "assert np.all(fused_z     == z)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b87fe6be",
   "metadata": {},
   "source": [
    "A standard assumption of many algorithms that use linear quantization is to set $\\mathtt{quantum\\_in} = \\mathtt{quantum\\_out}$.\n",
    "This assumption comes from the semantic interpretation of the quantizer as an approximator of a continuous function.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a69f9ac3",
   "metadata": {},
   "source": [
    "## Non-uniform quantization\n",
    "\n",
    "Some algorithms use non-uniformly sized gaps (i.e., the distances between two consecutive thresholds are not mutually equal) and non-uniformly spaced output values (i.e., the jumps between two consecutive quantization levels are not mutually equal).\n",
    "\n",
    "Usually, these algorithms compute the binning looking at the uni-variate distribution of the components of the array that they are meant to quantize.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8c3efc1a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from scipy.stats import truncnorm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def sample_from_univariate_gaussian_mixture(\n",
    "    ncomponents: int,\n",
    "    N: int,\n",
    "    minmi: float = -1.0,\n",
    "    maxmi: float = -1.0,\n",
    "    sigma_random_dilation_factor: float = 0.1) -> np.array:\n",
    "\n",
    "    # generate means\n",
    "    mis = (maxmi - minmi) * truncnorm.rvs(minmi, maxmi, size=ncomponents)\n",
    "    mis = np.sort(mis)\n",
    "\n",
    "    # generate standard deviations\n",
    "    sigmas = np.ones((ncomponents,)) + 0.2 * (2.0 * np.random.rand(ncomponents) - 1.0)\n",
    "\n",
    "    # generate component weights by partitioning the interval [0, 1)\n",
    "    _MAX = 100000\n",
    "    ps = np.sort((1 + np.random.permutation(_MAX - 1))[:ncomponents-1] / _MAX)\n",
    "    ps = np.concatenate((np.zeros((1,)), ps), axis=0)\n",
    "\n",
    "    # from which mixture component should each array component be drawn?\n",
    "    x_ps   = np.random.rand(N)\n",
    "    x_comp = np.zeros((N,), dtype=np.int64)\n",
    "    for i, p in enumerate(ps):\n",
    "        x_comp[x_ps > p] = i\n",
    "\n",
    "    # generate the array\n",
    "    x    = np.zeros((N,))\n",
    "    seed = np.random.randint(0, 1000)\n",
    "    rng  = np.random.RandomState(seed)\n",
    "    for i in range(0, nlevels):\n",
    "        x[x_comp == i] = rng.normal(loc=mis[i], scale=sigmas[i], size=N)[x_comp == i]\n",
    "        \n",
    "    return x\n",
    "\n",
    "\n",
    "nlevels = 8\n",
    "x = sample_from_univariate_gaussian_mixture(nlevels, N, minmi=-5.0, maxmi=7.0, sigma_random_dilation_factor=0.2)\n",
    "\n",
    "_ = plt.hist(x, bins='auto')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "138255f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "\n",
    "# generate the thresholds based on the statistics of the uni-variate distribution of the array\n",
    "kmeans = KMeans(n_clusters=nlevels)\n",
    "kmeans.fit(x.reshape(-1, 1))\n",
    "\n",
    "\n",
    "# thresholds and binning\n",
    "centroids = np.sort(kmeans.cluster_centers_.squeeze())\n",
    "thres     = (centroids[:-1] + centroids[1:]) / 2\n",
    "thres_ext = np.concatenate((np.array([-np.inf]), thres, np.array([np.inf])))\n",
    "\n",
    "bin_preimages = [(s, e) for s, e in zip(thres_ext[:-1], thres_ext[1:])]\n",
    "bin_ids       = np.arange(0, len(bin_preimages))\n",
    "bin2preimage  = {bin_id: preimage for bin_id, preimage in zip(bin_ids, bin_preimages)}\n",
    "bin2thres     = {bin_id: t for bin_id, t in zip(bin_ids[1:], thres)}\n",
    "\n",
    "\n",
    "# re-indexing\n",
    "bin2enc = {bin_id: bin_id for bin_id in bin_ids}  # it is the identity\n",
    "\n",
    "\n",
    "# de-integerisation\n",
    "outputs = centroids\n",
    "enc2out = {bin_id: out for bin_id, out in zip(bin_ids, outputs)}\n",
    "\n",
    "\n",
    "def binning(\n",
    "    x:            np.array,\n",
    "    bin2preimage: dict,\n",
    "    bin2thres:    dict) -> np.array:\n",
    "    \n",
    "    x_binned = np.zeros_like(x, dtype=np.int64)\n",
    "    \n",
    "    for bin_id, (s, e) in bin2preimage.items():\n",
    "        x_in_bin = np.logical_and(s < x, x < e)\n",
    "        x_binned[x_in_bin] = bin_id\n",
    "        \n",
    "    for bin_id, t in bin2thres.items():\n",
    "        x_binned[x == t] = bin_id\n",
    "        \n",
    "    return x_binned\n",
    "\n",
    "\n",
    "def reindexing(\n",
    "    x_binned: np.array,\n",
    "    bin2enc:  dict) -> np.array:\n",
    "\n",
    "    x_int = np.array(list(map(lambda x: bin2enc[x], x_binned)))\n",
    "\n",
    "    return x_int\n",
    "\n",
    "\n",
    "def deintegerisation(\n",
    "    x_int:   np.array,\n",
    "    enc2out: dict) -> np.array:\n",
    "    \n",
    "    z = np.array(list(map(lambda x: enc2out[x], x_int)))\n",
    "    \n",
    "    return z\n",
    "\n",
    "\n",
    "x_binned = binning(x, bin2preimage, thres2bin)\n",
    "x_int    = reindexing(x_binned, bin2enc)\n",
    "z        = deintegerisation(x_int, enc2out)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e38bc98",
   "metadata": {},
   "source": [
    "In these cases, the mapping from the binning range to the integer (i.e., encoding) range is usually the identity.\n",
    "In fact, the outputs are non-uniformly-spaced values that have no arithmetic relationship with the encodings (as happened for logarithmic and linear de-integerisation).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "834fe9fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build look-up table\n",
    "thres2enc = {t: bin2enc[bin_id] for bin_id, t in bin2thres.items()}\n",
    "\n",
    "\n",
    "def fused_binning_reindexing(\n",
    "    x:         np.array,\n",
    "    enc0:      int,\n",
    "    thres2enc: dict) -> np.array:\n",
    "\n",
    "    # this function works under the assumption that the bins are identified by increasing integers\n",
    "    x_int = np.full(x.shape, enc0)\n",
    "    for t, enc in thres2enc.items():\n",
    "        x_int[t <= x] = enc\n",
    "\n",
    "    return x_int\n",
    "\n",
    "fused_x_int = fused_binning_reindexing(x, encodings[0], bin2thres)\n",
    "fused_z     = deintegerisation(x_binned, enc2out)\n",
    "\n",
    "assert np.all(z == fused_z)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd151062",
   "metadata": {},
   "source": [
    "Non-uniform quantization schemes are the one that benefit the most from *compression algorithms*.\n",
    "For example, a statistical analysis on the distribution of the encodings of an array components can be used to apply Huffman coding to optimise storage savings.\n",
    "\n",
    "Therefore, we also think that it is possible to expand the three-step decomposition outlined above by adding two steps in-between re-indexing and de-integerisation:\n",
    "\n",
    "* **storage-oriented encoding**, going from the encoding range to an optimal storage-oriented encoding range (a set whose items are bit-strings);\n",
    "* **storage-oriented decoding**, going from the storage-oriented decoding range back to the encoding range.\n",
    "\n",
    "Of course, teh storage-oriented encoding function must be invertible, its inverse being the storage-oriented decoding function.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3a652f0",
   "metadata": {},
   "source": [
    "## Concluding remarks\n",
    "\n",
    "Analysing the four examples above, it seems that the proposed quantization algorithm use three strategies for optimising the binning step:\n",
    "\n",
    "* threshold-based/look-up table (LUT): $k$-means, ANA;\n",
    "* flooring: STE, PACT, SAWB;\n",
    "* logarithmic (most-significant bit, MSB): INQ.\n",
    "\n",
    "Similarly, we can identify three analogous strategies to perform the de-integerisation step:\n",
    "\n",
    "* look-up table (LUT): $k$-means;\n",
    "* linear scaling: STE, PACT, SAWB, ANA;\n",
    "* exponentiation: INQ.\n",
    "\n",
    "Based on this discussion, it seems reasonable to propose a format of specification of quantization algorithms such that:\n",
    "\n",
    "* its algorithmic implementation uses the operations that the programmer thinks are the most efficient to implement the desired behaviour;\n",
    "* its documentation maps the \"compact\" implementation into the described theoretical format, breaking down the implementation into its composing steps and exposing which of them have been \"fused\";\n",
    "* the PyTorch modules implementing the quantization algorithm must explicitly encode two pieces of information that will represent the annotation that is to be consumed by graph manipulation and code generation tools:\n",
    "\n",
    "  * the **semantics of the encodings** (are they to be used as bit-shifts, integer multipliers, or simply keys in a look-up table?);\n",
    "  * a **flag indicating whether the de-integerisation operation should be performed or not** (i.e., the encodings will be used \"as-they-are\").\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "605b039b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

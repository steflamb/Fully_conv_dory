{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "384016b1",
   "metadata": {},
   "source": [
    "# Quantizing a neural network using the *additive noise annealing* algorithm\n",
    "\n",
    "In this notebook we will introduce the `quantlib` tools that enable the structural transformations inherent to the training and deployment of a *quantized neural network* (QNN):\n",
    "\n",
    "* floating-point to fake-quantized (*float2fake*);\n",
    "* fake-quantized to true-quantized (*fake2true*).\n",
    "\n",
    "In this notebook we will not discuss the training process itself.\n",
    "\n",
    "\n",
    "## Post-training quantization algorithms vs. quantization-aware training algorithms\n",
    "\n",
    "Quantization algorithms for deep neural networks come in two flavours:\n",
    "\n",
    "* *post-training quantization* algorithms: these algorithms take in input trained floating-point networks and output quantized counterparts that use quantized operands;\n",
    "* *quantization-aware training algorithms: at training time, these algorithms map quantized values into floating-point numeric ranges to ease learning; after training, specific arithmetic conversions allow to transform the trained floating-point program into a quantized program.\n",
    "\n",
    "Post-training quantization algorithms are desirable for several reasons, the most important ones being data privacy and ease of application.\n",
    "The main drawback of post-training quantization algorithms are that they do not work well when involving \"aggressive\" quantization schemes (i.e., quantization schemes using less than four bits per operand): in such cases, the statistical accuracy of the quantized networks drop significantly with respect to that of the original floating-point programs.\n",
    "\n",
    "For this reason, when targetting extremely resource-constrained computers that benefit from using specialised accelerators, quantization-aware training algorithms are the preferred choice.\n",
    "\n",
    "In this notebook we will focus on a quantization-aware training algorithm called *additive noise annealing* (ANA).\n",
    "\n",
    "\n",
    "## The structure of the `quantlib` package\n",
    "\n",
    "The `quantlib.algorithms` package consists of several sub-packages providing the building blocks to build DNNs that can be trained using state-of-the-art quantization-aware training algorithms.\n",
    "\n",
    "Of course, the user can manually define new PyTorch networks using such building blocks.\n",
    "Sometimes, though, before implementing a brand-new DNN topology by hand it might be worth it to try and quantize an existing floating-point topology that has shown good performance at the target task.\n",
    "In this case, the network's PyTorch definition might already be available, and it is preferable to replace its floating-point operations with counterparts that support quantization in an automated way, without the hazards inherent to manual rewriting.\n",
    "The facilities to perform such *float2fake* conversions are implemented in the `quantlib.editing.lightweight` sub-package.\n",
    "\n",
    "After training, a fake-quantized network is in general not yet ready to be executed on energy-efficient hardware.\n",
    "This limitation is due to several reasons.\n",
    "Some examples:\n",
    "\n",
    "* the quantized operands (weights, features, or both) are \"immersed\" in some floating-point range, a numeric representation that nullifies the benefits of quantization;\n",
    "* batch normalisation operations, where the channel means, standard deviations, weights and biases are represented by floating-point numbers, are still executed.\n",
    "\n",
    "Enabling execution on energy-efficient hardware requires the user to transform the program in such a way that these operations are removed, while at the same time preserving the functional mapping performed.\n",
    "This *fake2true* conversion process usually necessitates a complete overview of the low-level operations that are executed under the hood by a PyTorch fake-quantized networks, including their connectivity and the data format (type and precision) of their operands.\n",
    "Since PyTorch build computational graphs dynamically, deriving information about the connectivity requires *tracing* a fake-quantized graph.\n",
    "After this step, *graph rewriting* and *arithmetic conversion* will be involved.\n",
    "The facilities to perform these actions are implemented in the `quantlib.editing.graphs` sub-package.\n",
    "\n",
    "Since QNNs are often going to target hardware accelerators with specific ISAs, the agent performing a *fake2true* conversion must be aware of the properties of the underlying platform, which in `quantlib` jargon we refer to as a `backend`.\n",
    "For this reason, some of the abstractions defined in `quantlib.editing.graphs` are sub-classed by entities defined in modules belonging to sub-packages of the `quantlib.backends` package.\n",
    "\n",
    "To summarise:\n",
    "\n",
    "* *float2fake* conversions take as inputs floating-point PyTorch DNNs and use the facilities defined in the `quantlib.editing.lighweight` package to return fake-quantized QNNs;\n",
    "* *fake2true* conversions take as inputs fake-quantized QNNs and use the facilities defined in the `quantlib.editing.graphs` and (some of) the facilities defined in the `quantlib.backends` package to return a software \"golden model\" of the program that will actually run on the target device.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb5de7ee",
   "metadata": {},
   "source": [
    "## Quantizing a VGG8 on CIFAR-10 with ANA targetting CUTIE\n",
    "\n",
    "As a guiding example, we will go through the process of quantizing a simple feedforward *convolutional neural network* (CNN) topology (VGG8) to process CIFAR-10 RGB images.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "75d3aa47",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "_CONFIGS = {\n",
    "    'VGG8': ['M', 256, 256, 'M', 512, 512, 'M'],\n",
    "    'VGG9': [128, 'M', 256, 256, 'M', 512, 512, 'M'],\n",
    "    'VGG11': [128, 'M', 256, 256, 256, 'M', 512, 512, 512, 'M'],\n",
    "}\n",
    "\n",
    "\n",
    "class VGG(nn.Module):\n",
    "\n",
    "    def __init__(self, config: str, capacity: int = 1, use_bn_features: bool = False, use_bn_classifier: bool = False, num_classes: int = 10, seed: int = -1) -> None:\n",
    "\n",
    "        super(VGG, self).__init__()\n",
    "\n",
    "        self.pilot      = self._make_pilot(capacity, use_bn_features)\n",
    "        self.features   = self._make_features(config, capacity, use_bn_features)\n",
    "        self.avgpool    = nn.AdaptiveAvgPool2d((4, 4))\n",
    "        self.classifier = self._make_classifier(capacity, use_bn_classifier, num_classes)\n",
    "\n",
    "        self._initialize_weights(seed=seed)\n",
    "\n",
    "    @staticmethod\n",
    "    def _make_pilot(capacity: int, use_bn_features: bool) -> nn.Sequential:\n",
    "\n",
    "        modules = []\n",
    "        modules += [nn.Conv2d(3, 128 * capacity, kernel_size=3, padding=1, bias=not use_bn_features)]\n",
    "        modules += [nn.BatchNorm2d(128 * capacity)] if use_bn_features else []\n",
    "        modules += [nn.ReLU(inplace=True)]\n",
    "\n",
    "        return nn.Sequential(*modules)\n",
    "\n",
    "    @staticmethod\n",
    "    def _make_features(config: str, capacity: int, use_bn_features: bool) -> nn.Sequential:\n",
    "\n",
    "        modules = []\n",
    "        in_channels = 128 * capacity\n",
    "        for v in _CONFIGS[config]:\n",
    "            if v == 'M':\n",
    "                modules += [nn.MaxPool2d(kernel_size=2, stride=2)]\n",
    "            else:\n",
    "                out_channels = v * capacity\n",
    "                modules += [nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1, bias=not use_bn_features)]\n",
    "                modules += [nn.BatchNorm2d(out_channels)] if use_bn_features else []\n",
    "                modules += [nn.ReLU(inplace=True)]\n",
    "                in_channels = out_channels\n",
    "\n",
    "        return nn.Sequential(*modules)\n",
    "\n",
    "    @staticmethod\n",
    "    def _make_classifier(capacity: int, use_bn_classifier: bool, num_classes: int) -> nn.Sequential:\n",
    "\n",
    "        modules = []\n",
    "        modules += [nn.Linear(512 * capacity * 4 * 4, 1024, bias=not use_bn_classifier)]\n",
    "        modules += [nn.BatchNorm1d(1024)] if use_bn_classifier else []\n",
    "        modules += [nn.ReLU(inplace=True)]\n",
    "        modules += [] if use_bn_classifier else [nn.Dropout()]\n",
    "        modules += [nn.Linear(1024, 1024, bias=not use_bn_classifier)]\n",
    "        modules += [nn.BatchNorm1d(1024)] if use_bn_classifier else []\n",
    "        modules += [nn.ReLU(inplace=True)]\n",
    "        modules += [] if use_bn_classifier else [nn.Dropout()]\n",
    "        modules += [nn.Linear(1024, num_classes)]\n",
    "\n",
    "        return nn.Sequential(*modules)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "\n",
    "        x = self.pilot(x)\n",
    "        x = self.features(x)\n",
    "        x = self.avgpool(x)\n",
    "\n",
    "        x = x.view(x.size(0), -1)  # https://stackoverflow.com/questions/57234095/what-is-the-difference-of-flatten-and-view-1-in-pytorch\n",
    "\n",
    "        x = self.classifier(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def _initialize_weights(self, seed: int):\n",
    "\n",
    "        if seed >= 0:\n",
    "            torch.manual_seed(seed)\n",
    "\n",
    "        for m in self.modules():\n",
    "\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                nn.init.normal_(m.weight, 0, 0.01)\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c6074c9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "net = VGG('VGG8', use_bn_features=True, use_bn_classifier=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f435970f",
   "metadata": {},
   "source": [
    "We aim at quantizing both its weights and features to ternary values (*ternary neural network*, TNN), and at deriving a software golden model for a program running on the *completely unrolled ternary inference engine* (CUTIE) TNN accelerator.\n",
    "\n",
    "The chosen quantization-aware training algorithm is the *additive noise annealing* (ANA) algorithm.\n",
    "Loosely speaking, this algorithm replaces non-differentiable quantizers with almost-everywhere differentiable counterparts obtained by applying the expectation operator to quantizers that take noisy inputs.\n",
    "As the training algorithm progresses, the noise distribution is collapsed to a Dirac's delta, resulting in the differentiable function being \"frozen\" into the non-differentiable quantizer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bea49c1b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAD4CAYAAADhNOGaAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8QVMy6AAAACXBIWXMAAAsTAAALEwEAmpwYAAAXZElEQVR4nO3de7BV5X3G8e/jASQijSIHgkCEQUpCmmjtKdXaMdqIBaYNsW1moB1DLw61IzPR3kJjJ3F6tc2k6bSxMjRhJJ1EkjRaaEKCl2mGtiYpR4sIKuGAWI6gbIGqXCwe/PWPvaB7H/e5rrXP2uus5zOz512X9937twfhca2113oVEZiZWXmdl3cBZmaWLweBmVnJOQjMzErOQWBmVnIOAjOzkhuTdwHDMXny5Jg1a1beZZiZFcoTTzzxSkS0995eyCCYNWsWnZ2deZdhZlYokl5otN2nhszMSs5BYGZWcg4CM7OScxCYmZWcg8DMrOQyCQJJ6yQdlrSzj/2S9LeSuiTtkHRVzb5FknYn+1ZnUY+ZmQ1eVkcE9wOL+tm/GJibvFYC9wFIagPuTfbPB5ZLmp9RTWZmNgiZ3EcQEVslzeqny1LgS1F95vX3JV0kaRowC+iKiH0AkjYkfZ/Joi4zax13b3ya+7/333mXUXjtE8ay5XeuZ9KEcZm950hdI5gOHKhZ70629bX9bSStlNQpqbNSqTStUDPL3p0PPOEQyEjlxJt8vfPAwB2HYKSCQA22RT/b374xYm1EdERER3v72+6QNrMW9eQLx3joqZfyLmPUaJ8wlo92zMz0PUfqERPdQG3lM4CDwLg+tpvZKPFb/1j/OJhfu+bd3L30/TlVY42M1BHBJuBjya+HrgZejYhDwDZgrqTZksYBy5K+ZjZKVI6fPrf8R0ve4xBoQZkcEUh6ALgemCypG/g0MBYgItYAm4ElQBdwEvj1ZF+PpFXAFqANWBcRu7Koycxag/j/88C3Xjcn52qskax+NbR8gP0B3N7Hvs1Ug8LMRpmjJ05zHnAGOL+QzzouB99ZbGZN8zeP/JAzyfLcKRNzrcX65iAws6Z5+Jnqr4XObxN/s/yqAXpbXhwEZtY0PWfeAmDi+DHMab8w52qsLw4CM2saJbcKqeEtQ9YqHARm1jTtE8fVtdaaHARm1jRj2s6ra601+U/HzKzkHARmZiXnIDCzpthbOc7+oycAGD+2LedqrD8OAjNrik9v3MVrp85w8QVj+Ytf+kDe5Vg/HARm1hSXXjQegIXzp/oeghbnIDCzpug6fLyutdblIDAzKzkHgZk1xeVTLqxrrXU5CMysKV44crKutdblIDCzppg3dWJda60rkyCQtEjSbkldklY32P/7krYnr52SzkialOzbL+npZF/n29/dzApJvVprWannDJLUBtwLLKQ6Sf02SZsi4pmzfSLiM8Bnkv6/ANwZEUdr3uaGiHglbS1m1jp2v/R6XWutK4sjggVAV0Tsi4jTwAZgaT/9lwMPZPC5ZtbCfGqoOLIIgunAgZr17mTb20i6AFgEfKNmcwAPS3pC0sq+PkTSSkmdkjorlUoGZZtZM519rIQfL9H6sgiCRmcAo4++vwD8R6/TQtdGxFXAYuB2Sdc1GhgRayOiIyI62tvb01VsZk33Rs+ZutZaVxZB0A3MrFmfARzso+8yep0WioiDSXsYeIjqqSYzK7CjJ06z9YfVI/e9vrO45WURBNuAuZJmSxpH9R/7Tb07SXon8EFgY822CZImnl0GbgJ2ZlCTmeVo/eP72X/kJLMuuYA//siP5V2ODSD1r4YiokfSKmAL0Aasi4hdkm5L9q9Jut4MPBwRJ2qGTwUeknS2lq9ExHfS1mRm+Tp1uno66Kb57/ID5wogdRAARMRmYHOvbWt6rd8P3N9r2z7giixqMDOz4fGdxWaWOV8oLhYHgZllzjeTFYuDwMwy55vJisVBYGaZ881kxeIgMDMrOQeBmVnJOQjMzErOQWBmVnIOAjOzknMQmJmVnIPAzKzkHARmZiXnIDAzKzkHgZlZyTkIzMxKzkFgZpk6euI0uw6+CsA7xvmfmCLI5E9J0iJJuyV1SVrdYP/1kl6VtD15fWqwY82sWNY/vp//2HuEn7l8Mit+enbe5dggpJ6hTFIbcC+wkOpE9tskbYqIZ3p1/beI+PlhjjWzgjg7TeX8aT/CpAnjcq7GBiOLI4IFQFdE7IuI08AGYOkIjDUzswxkEQTTgQM1693Jtt6ukfSUpG9Let8QxyJppaROSZ2VSiWDss3MDLIJAjXYFr3WnwQui4grgL8D/nkIY6sbI9ZGREdEdLS3tw+3VjMz6yWLIOgGZtaszwAO1naIiNci4niyvBkYK2nyYMaamVlzZREE24C5kmZLGgcsAzbVdpD0LklKlhckn3tkMGPNzKy5Uv9qKCJ6JK0CtgBtwLqI2CXptmT/GuCXgd+W1AOcApZFRAANx6atyczMBi91EMC50z2be21bU7P8eeDzgx1rZmYjx7f9mVmm3ug5U9da63MQmFmmdr/0el1rrc9BYGaZmjd1Yl1rrc9BYGaZGj+2ra611ucgMDMrOQeBmVnJOQjMzErOQWBmVnIOAjOzknMQmFlmPE1lMflPyswy42kqi8lBYGaZ8TSVxeQgMLPM+DlDxeQgMLPM+DlDxeQgMLPM+DlDxZRJEEhaJGm3pC5Jqxvs/1VJO5LX45KuqNm3X9LTkrZL6syiHjPLh58zVEypJ6aR1AbcCyykOgfxNkmbIuKZmm7PAx+MiGOSFgNrgZ+q2X9DRLySthYzMxu6LI4IFgBdEbEvIk4DG4CltR0i4vGIOJasfp/qJPVmNsr4YnExZREE04EDNevdyba+/Cbw7Zr1AB6W9ISklX0NkrRSUqekzkqlkqpgM2sOXywupiyCQA22RcOO0g1Ug+ATNZuvjYirgMXA7ZKuazQ2ItZGREdEdLS3t6et2cyawBeLiymLIOgGZtaszwAO9u4k6QPAF4ClEXHk7PaIOJi0h4GHqJ5qMrMiUq/WCiGLINgGzJU0W9I4YBmwqbaDpHcDDwK3RMQPa7ZPkDTx7DJwE7Azg5rMLAc+NVRMqX81FBE9klYBW4A2YF1E7JJ0W7J/DfAp4BLg7yUB9EREBzAVeCjZNgb4SkR8J21NZpaPeVMn8oPnj/rUUMGkDgKAiNgMbO61bU3N8q3ArQ3G7QOu6L3dzArKp4YKyXcWm1lmfGqomBwEZpaJvZXjHPyfUwBcMeOdOVdjQ+EgMLNMfPLBpzlw7BQzL34Ht11/ed7l2BA4CMwsE2+eeQuA9onney6CgnEQmFkmLp9yYV1rxeEgMLNMdB0+XtdacTgIzCwTPcmpobOtFYeDwMys5BwEZmYl5yAws0z41FBxOQjMLLW9lePnLhL/z6menKuxoXIQmFlqv//1pzidHAjc+N4p+RZjQ+YgMLPUuo+eBGDCuDbuWDgv52psqBwEZpZaz1vVSQnPH3Oe7youIAeBmaV26s2eutaKxUFgZqnsrRzn1JvVI4KIhtOVW4vLJAgkLZK0W1KXpNUN9kvS3yb7d0i6arBjzay13Xr/tnPLbW2ekaaIUgeBpDbgXmAxMB9YLml+r26LgbnJayVw3xDGmlmL+pftL/L8kZPn1u9d3pFjNTZcWUxVuQDoSqadRNIGYCnwTE2fpcCXonrc+H1JF0maBswaxFiz0lr9T9vZ0Pli3mUM2vXv8U9HiyiLU0PTgQM1693JtsH0GcxYACStlNQpqbNSqaQu2qzV/eXmZwoVAr92zbvzLsGGKYsgaHRSsPcVo776DGZsdWPE2ojoiIiO9vb2IZZoVjz3bX0+7xIG7cZ5k7l76fvzLsOGKYtTQ93AzJr1GcDBQfYZN4ixZqXz3ecO163/9nWz+cQSXz6z5sjiiGAbMFfSbEnjgGXApl59NgEfS349dDXwakQcGuRYs9K5/YEnzy3PeOf5DgFrqtRHBBHRI2kVsAVoA9ZFxC5JtyX71wCbgSVAF3AS+PX+xqatyazo3vjfM+eW1996dY6VWBlkcWqIiNhM9R/72m1rapYDuH2wY83KbmwbnDkD49tgTrvnALbm8p3FZi3ogvPH1LVmzeQgMGsxT75wjGMnq8/sGT/WQWDN5yAwazEf3/BfBDDmPPH5X7lqwP5maTkIzFrMxPPbAPjRKRO46rKLc67GysBBYGZWcg4CsxZz5MSbda1ZszkIzFrMJRPG1rVmzeYgMGsx48eNqWvNms1BYNZiLp9yYV1r1mwOArMW03X4eF1r1mwOAjOzknMQmLWQvZXjVF7/XwA6fA+BjRAHgVkL+fTGXRw4dopZl1zAbddfnnc5VhIOArMWculF4wFYMHsSkyaMy7kaKwsHgVkLeeHIybrWbCQ4CMxayGWXXFDXmo2EVEEgaZKkRyTtSdq3Xd2SNFPSv0p6VtIuSR+v2Xe3pBclbU9eS9LUY1Z0PiKwPKQ9IlgNPBYRc4HHkvXeeoDfjYj3AlcDt0uqnYD1cxFxZfLyTGVWavOmTqxrzUZC2iBYCqxPltcDH+ndISIORcSTyfLrwLPA9JSfazYqjR/bVteajYS0QTA1Ig5B9R98YEp/nSXNAn4c+EHN5lWSdkha1+jUUs3YlZI6JXVWKpWUZZu1pjd6ztS1ZiNhwCCQ9KiknQ1eS4fyQZIuBL4B3BERryWb7wPmAFcCh4DP9jU+ItZGREdEdLS3tw/lo80KY/dLr9e1ZiNhwMcbRsSNfe2T9LKkaRFxSNI04HAf/cZSDYEvR8SDNe/9ck2ffwC+OZTizUabeVMn8oPnj/oagY2otKeGNgErkuUVwMbeHSQJ+CLwbET8da9902pWbwZ2pqzHrNB8jcDykDYI7gEWStoDLEzWkXSppLO/ALoWuAX42QY/E/0rSU9L2gHcANyZsh4zMxuiVDNfRMQR4EMNth8EliTL/w6oj/G3pPl8s9HGF4stD76z2KyF+GKx5cFBYNYijp44TUQAcMWMd+ZcjZWJg8CsRax/fD//uf8YP3P5ZD+C2kaUg8CsRZw6Xb0uMH/aj/gR1DaiHARmZiXnIDAzKzkHgZlZyTkIzMxKzkFgZlZyDgIzs5JzEJiZlZyDwMys5BwEZmYl5yAwMys5B4GZWcmlCgJJkyQ9ImlP0jacfF7S/mQCmu2SOoc63szMmiftEcFq4LGImAs8lqz35YaIuDIiOoY53szMmiBtECwF1ifL64GPjPB4s1HDs5NZXtIGwdSIOASQtFP66BfAw5KekLRyGOORtFJSp6TOSqWSsmyz1uPZySwvA85ZLOlR4F0Ndt01hM+5NiIOSpoCPCLpuYjYOoTxRMRaYC1AR0dHDGWsWavz7GSWpwGDICJu7GufpJclTYuIQ5KmAYf7eI+DSXtY0kPAAmArMKjxZqOdZyezPKU9NbQJWJEsrwA29u4gaYKkiWeXgZuAnYMdb1YGnp3M8pQ2CO4BFkraAyxM1pF0qaTNSZ+pwL9Legr4T+BbEfGd/sabmdnIGfDUUH8i4gjwoQbbDwJLkuV9wBVDGW9mZiPHdxabmZWcg8DMrOQcBGZmJecgMDMrOQeBmVnJOQjMzErOQWBmVnIOAjOzknMQmJmVnIPAzKzkHARmZiXnIDAzKzkHgVkL8DSVlicHgVkL8DSVlicHgVnOPE2l5c1BYJYzT1NpeUsVBJImSXpE0p6kvbhBn3mStte8XpN0R7Lvbkkv1uxbkqYesyLyNJWWt7RHBKuBxyJiLvBYsl4nInZHxJURcSXwE8BJ4KGaLp87uz8iNvcebzba+UKx5S1tECwF1ifL64GPDND/Q8DeiHgh5eeajRq+UGx5SxsEUyPiEEDSThmg/zLggV7bVknaIWldo1NLZ0laKalTUmelUklXtVkLmTd1Yl1rNtIGDAJJj0ra2eC1dCgfJGkc8GHg6zWb7wPmAFcCh4DP9jU+ItZGREdEdLS3tw/lo81a2vixbXWt2UgbM1CHiLixr32SXpY0LSIOSZoGHO7nrRYDT0bEyzXvfW5Z0j8A3xxc2Wajh68RWN7SnhraBKxIllcAG/vpu5xep4WS8DjrZmBnynrMCsfXCCxvaYPgHmChpD3AwmQdSZdKOvcLIEkXJPsf7DX+ryQ9LWkHcANwZ8p6zArnsksuqGvNRtqAp4b6ExFHqP4SqPf2g8CSmvWTwCUN+t2S5vPNRoMXjpysa81Gmu8sNsuZjwgsbw4Cs5z5iMDy5iAwy5mPCCxvDgKzHB09cZrv7T0C+IjA8uMgMMvR+sf3c+DYKcCPoLb8OAjMcnTs5GkArnr3RX4EteXGQWCWo50vvnpu2Y+gtrw4CMxy9MbpnrrWLA8OArMcHTnxZl1rlgcHgVmOLpkwtq41y4ODwMys5BwEZjl65fjputYsDw4Cs5zsrRzn2IlqAEjKuRorMweBWU7u/Op23ozq8s+9b2q+xVipOQjMcrLn5dcAGHMe3LFwXs7VWJk5CMxy8kZyOHDmLd9MZvlKFQSSPippl6S3JHX002+RpN2SuiStrtk+SdIjkvYk7cVp6jErii9s3UtyVuhca5aXtEcEO4FfBLb21UFSG3Av1cnr5wPLJc1Pdq8GHouIucBjybrZqPYv21/kTzc/d259brsfP235SjtV5bMw4C8eFgBdEbEv6bsBWAo8k7TXJ/3WA98FPpGmpv58Yeveur+AZq1gzcd+Mu8SrORG4hrBdOBAzXp3sg1gakQcAkjaKX29iaSVkjoldVYqlWEV8uffdghYa1nyvinMab8w7zKs5AYMAkmPStrZ4LV0kJ/R6HBhyKdFI2JtRHREREd7e/tQhwPwycXvGdY4s2a4bs4k/v4WHw1Y/gY8NRQRN6b8jG5gZs36DOBgsvyypGkRcUjSNOBwys/q163XzeHW6+Y08yPMzApnJE4NbQPmSpotaRywDNiU7NsErEiWVwAbR6AeMzOrkfbnozdL6gauAb4laUuy/VJJmwEiogdYBWwBngW+FhG7kre4B1goaQ+wMFk3M7MRpIji/Yq5o6MjOjs78y7DzKxQJD0REW+758t3FpuZlZyDwMys5BwEZmYl5yAwMyu5Ql4sllQBXhjm8MnAKxmWkyd/l9YzWr4H+Lu0qjTf5bKIeNsduYUMgjQkdTa6al5E/i6tZ7R8D/B3aVXN+C4+NWRmVnIOAjOzkitjEKzNu4AM+bu0ntHyPcDfpVVl/l1Kd43AzMzqlfGIwMzMajgIzMxKrtRBIOn3JIWkyXnXMlyS/kTSDknbJT0s6dK8axoOSZ+R9FzyXR6SdFHeNQ2XpI9K2iXpLUmF/MmipEWSdkvqklTYucQlrZN0WNLOvGtJQ9JMSf8q6dnkv62PZ/n+pQ0CSTOpPvr6v/OuJaXPRMQHIuJK4JvAp3KuZ7geAX4sIj4A/BD4w5zrSWMn8IvA1rwLGQ5JbcC9wGJgPrBc0vx8qxq2+4FFeReRgR7gdyPivcDVwO1Z/pmUNgiAzwF/wDCmzWwlEfFazeoECvp9IuLhZO4KgO9TncmukCLi2YjYnXcdKSwAuiJiX0ScBjYAg52atqVExFbgaN51pBURhyLiyWT5dapzu0zvf9TgDThV5Wgk6cPAixHxlNRoSuVikfRnwMeAV4Ebci4nC78BfDXvIkpsOnCgZr0b+KmcarFeJM0Cfhz4QVbvOWqDQNKjwLsa7LoL+CRw08hWNHz9fZeI2BgRdwF3SfpDqrPBfXpECxykgb5H0ucuqofBXx7J2oZqMN+lwBr931EhjzRHG0kXAt8A7uh1NiCVURsEEXFjo+2S3g/MBs4eDcwAnpS0ICJeGsESB62v79LAV4Bv0aJBMND3kLQC+HngQ9HiN7gM4c+kiLqBmTXrM4CDOdViCUljqYbAlyPiwSzfe9QGQV8i4mlgytl1SfuBjogo5JMJJc2NiD3J6oeB5/KsZ7gkLQI+AXwwIk7mXU/JbQPmSpoNvAgsA34l35LKTdX/a/0i8GxE/HXW71/mi8WjxT2SdkraQfV0V6Y/KxtBnwcmAo8kP4Vdk3dBwyXpZkndwDXAtyRtybumoUgu2q8CtlC9KPm1iNiVb1XDI+kB4HvAPEndkn4z75qG6VrgFuBnk78f2yUtyerN/YgJM7OS8xGBmVnJOQjMzErOQWBmVnIOAjOzknMQmJmVnIPAzKzkHARmZiX3f8cWnZ24sVVRAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import quantlib.algorithms as qa\n",
    "\n",
    "\n",
    "act = qa.ana.ANAActivation({'nbits': 2, 'signed': True, 'balanced': True, 'eps': 1.0}, 'normal')\n",
    "\n",
    "x = torch.arange(-4.0, 2.0, step=0.001).to(dtype=torch.float32)\n",
    "x.requires_grad = False\n",
    "\n",
    "for lambda_ in torch.arange(0.8, -0.1, step=-0.2):\n",
    "\n",
    "    act.set_fnoise(-lambda_, (-lambda_) ** 2)\n",
    "    y = act(x)\n",
    "\n",
    "    plt.scatter(x, y, s=1)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55811aa8",
   "metadata": {},
   "source": [
    "## Converting a floating-point graph into a fake-quantized graph\n",
    "\n",
    "As described in the introduction, the `quantlib.editing.lightweight` sub-package provides the functionality to perform tree traversal and leaf replacement.\n",
    "In this section we will illustrate the main abstractions implemented in the package and how the user can use them to transform a floating-point PyTorch network into a fake-quantized one.\n",
    "\n",
    "We start by importing the corresponding namespace.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "43e16296",
   "metadata": {},
   "outputs": [],
   "source": [
    "import quantlib.editing.lightweight as qlw"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "856ef7ca",
   "metadata": {},
   "source": [
    "### DNNs in PyTorch\n",
    "\n",
    "In PyTorch, users can define DNNs as sub-classes of `torch.nn.Module`.\n",
    "We can distinguish between two main flavours of PyTorch modules:\n",
    "\n",
    "* *basic* modules;\n",
    "* *container* modules.\n",
    "\n",
    "Basic modules represent primitive transformations.\n",
    "Essentially, they are Python wrappers for operations that support automatic differentiation, hence enabling the execution of the back-propagation algorithm.\n",
    "Each basic module is characterised by a *type*\n",
    "\n",
    "Container modules are, well, containers for other PyTorch modules.\n",
    "The purpose of container modules is structuring the execution of their children modules.\n",
    "For instance, the purpose of a `torch.nn.Sequential` module is creating a composable pipeline of transformations that should be executed one after the other.\n",
    "The definition of a container module is recursive, in that the modules that it contains can be either container themselves or basic modules.\n",
    "In PyTorch, DNN objects are almost always containers.\n",
    "\n",
    "Due to the recursive nature of PyTorch modules, a PyTorch network object can be represented as a **tree** data structure.\n",
    "The root and intermediate nodes of such a tree are container modules, whereas its leaves are basic modules.\n",
    "Moreover, an alphanumeric string is attached to each node, allowing to refer them by *name*.\n",
    "\n",
    "\n",
    "### `LightweightGraph`\n",
    "\n",
    "Despite its name, a `LightweightGraph` is a wrapper structure around a `torch.nn.Module` object that provides a key functionality: the `rebuild_nodes_list` method traverses the tree of `torch.nn.Module`s and returns a list of `LightweightNode`.\n",
    "Each `LightweightNode` is a pair composed by:\n",
    "\n",
    "* a string concatenating the names of each traversed node, separated by dots;\n",
    "* a reference to a non-container `torch.nn.Module`.\n",
    "\n",
    "The list of `LightweightNode` (a private attribute denoted by `_nodes_list`) can be inspected using the method `show_nodes_list`.\n",
    "Instead of printing the reference to the `torch.nn.Module` component, its type will be printed instead; this design decision is important since in the context of of *float2fake* conversion we aim at replacing `torch.nn.Module` of specific types with fake-quantized counterparts.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2dd7e1d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pilot.0                        <class 'torch.nn.modules.conv.Conv2d'>\n",
      "pilot.1                        <class 'torch.nn.modules.batchnorm.BatchNorm2d'>\n",
      "pilot.2                        <class 'torch.nn.modules.activation.ReLU'>\n",
      "features.0                     <class 'torch.nn.modules.pooling.MaxPool2d'>\n",
      "features.1                     <class 'torch.nn.modules.conv.Conv2d'>\n",
      "features.2                     <class 'torch.nn.modules.batchnorm.BatchNorm2d'>\n",
      "features.3                     <class 'torch.nn.modules.activation.ReLU'>\n",
      "features.4                     <class 'torch.nn.modules.conv.Conv2d'>\n",
      "features.5                     <class 'torch.nn.modules.batchnorm.BatchNorm2d'>\n",
      "features.6                     <class 'torch.nn.modules.activation.ReLU'>\n",
      "features.7                     <class 'torch.nn.modules.pooling.MaxPool2d'>\n",
      "features.8                     <class 'torch.nn.modules.conv.Conv2d'>\n",
      "features.9                     <class 'torch.nn.modules.batchnorm.BatchNorm2d'>\n",
      "features.10                    <class 'torch.nn.modules.activation.ReLU'>\n",
      "features.11                    <class 'torch.nn.modules.conv.Conv2d'>\n",
      "features.12                    <class 'torch.nn.modules.batchnorm.BatchNorm2d'>\n",
      "features.13                    <class 'torch.nn.modules.activation.ReLU'>\n",
      "features.14                    <class 'torch.nn.modules.pooling.MaxPool2d'>\n",
      "avgpool                        <class 'torch.nn.modules.pooling.AdaptiveAvgPool2d'>\n",
      "classifier.0                   <class 'torch.nn.modules.linear.Linear'>\n",
      "classifier.1                   <class 'torch.nn.modules.batchnorm.BatchNorm1d'>\n",
      "classifier.2                   <class 'torch.nn.modules.activation.ReLU'>\n",
      "classifier.3                   <class 'torch.nn.modules.linear.Linear'>\n",
      "classifier.4                   <class 'torch.nn.modules.batchnorm.BatchNorm1d'>\n",
      "classifier.5                   <class 'torch.nn.modules.activation.ReLU'>\n",
      "classifier.6                   <class 'torch.nn.modules.linear.Linear'>\n"
     ]
    }
   ],
   "source": [
    "lwg = qlw.LightweightGraph(net)\n",
    "lwg.show_nodes_list()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11d03a17",
   "metadata": {},
   "source": [
    "### `Filter`s and  `LightweightRule`s\n",
    "\n",
    "In the scope of *float2fake* conversion, it is important that we are able to replace leaf nodes selectively.\n",
    "Such replacements act in two steps:\n",
    "\n",
    "* identifying the *application points*, i.e., the `LightweightNode`s that satisfy some predefined criteria;\n",
    "* performing the *replacement action*.\n",
    "\n",
    "For the purpose of identifying application points, the `qlw.rules.filters` module implements `Filter` objects that can be applied to lists of `LightweightNode`s.\n",
    "\n",
    "There are two types of *basic* filters:\n",
    "\n",
    "* `NameFilter`s, that filter the input list by matching a regular expression against the first component of each item in the list;\n",
    "* `TypeFilter`s, that filter the input list using the type of the second component as a reference.\n",
    "\n",
    "*Composed* `Filter`s can be built using logical operators:\n",
    "\n",
    "* `-` negation (unary);\n",
    "* `&` conjunction (either binary or variadic);\n",
    "* `|` disjunction (either binary or variadic).\n",
    "\n",
    "For example, to identify all the `torch.nn.Conv2d` PyTorch modules in the feature extractor part of the VGG network implemented above, we can define the following filters:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "503d00bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# basic type-based filters\n",
    "filter_conv2d = qlw.rules.filters.TypeFilter(nn.Conv2d)\n",
    "filter_linear = qlw.rules.filters.TypeFilter(nn.Linear)\n",
    "filter_relu   = qlw.rules.filters.TypeFilter(nn.ReLU)\n",
    "\n",
    "# basic name-based filters\n",
    "filter_pilot      = qlw.rules.filters.NameFilter('pilot')\n",
    "filter_features   = qlw.rules.filters.NameFilter('features')\n",
    "filter_classifier = qlw.rules.filters.NameFilter('classifier')\n",
    "\n",
    "# composed filter example\n",
    "filter_conv2d_pilot_or_features = filter_conv2d & (filter_pilot | filter_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "30957736",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features.11          <class 'torch.nn.modules.conv.Conv2d'>\n",
      "features.4           <class 'torch.nn.modules.conv.Conv2d'>\n",
      "features.8           <class 'torch.nn.modules.conv.Conv2d'>\n",
      "features.1           <class 'torch.nn.modules.conv.Conv2d'>\n",
      "pilot.0              <class 'torch.nn.modules.conv.Conv2d'>\n"
     ]
    }
   ],
   "source": [
    "for n in filter_conv2d_pilot_or_features(lwg.nodes_list):\n",
    "    print(\"{:20s} {}\".format(n.name, n.module.__class__))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ba9a66b",
   "metadata": {},
   "source": [
    "Once a `Filter` has been defined, it must be coupled with a suitable replacement function that can implement the desired leaf replacement.\n",
    "For example, we would like to replace `torch.nn.Conv2d` PyTorch modules with `quantlib.algorithms.ana.ANAConv2d` counterparts.\n",
    "This functionality is performed by `LightweightRule`s.\n",
    "\n",
    "Whereas `TypeFilter`s can be applied to generic PyTorch networks, we can not make the same assumption for `NameFilter`s since module names depend on the definition of the class of the specific network that is the subject of the *float2fake* conversion.\n",
    "Thus, to ensure generality we assume that `Filter`s are specific to the network object.\n",
    "\n",
    "On the contrary, the quality of the replacement depends just on the type of the `torch.nn.Module`s returned by the application of a `Filter` and of the fake-quantized modules that are goint to replace it.\n",
    "This network-independent part of a `LightweightRule` is implemented by the so-called `replacement_fun` attribute of the rule.\n",
    "A replacement function takes as input a leaf PyTorch module (i.e., a non-container PyTorch module) and returns either a container PyTorch module containing basic fake-quantized PyTorch modules or a basic fake-quantized PyTorch module, as defined by the programmer.\n",
    "Since most quantization-aware training algorithms condition their behaviour on some hyper-parameters, replacement functions can take them as arguments.\n",
    "\n",
    "A custom `LightweightRule` in then defined according to this procedure:\n",
    "\n",
    "* the programmer implements the `replacement_fun` in such a way to be agnostic of the networks it will be applied to; the hyper-parameters of the fake-quantized modules are left as arguments; in conjunction with currying, this coding practice can act in a similar way as templatisation in languages such as C++;\n",
    "* the programmer implements the custom `LightweightRule` by sub-classing the stock `LightweightRule` class; the constructor method of this class should take as inputs actual values for the fake-quantized modules hyper-parameters and use currying (as provided by Python's `functools` package) to instantiate a replacement function from the \"template\"; also, the constructor method should take a `Filter` as input.\n",
    "\n",
    "In this way, a user can instantiate a very specific leaf replacement rule by passing the `Filter` that identifies the desired application points in the subject network and the hyper-paremeters for the fake-quantized modules associated with the chosen quantization-aware training algorithm.\n",
    "\n",
    "The behaviour inherited by the stock `LightweightRule` class is such that applying a custom rule will perform the following actions:\n",
    "\n",
    "* take as input a `LightweightGraph`s;\n",
    "* apply the `Filter` to its `nodes_list` and return the target leaves;\n",
    "* for each target leaf, traverse the tree and replace its module with the one specified by the `replacement_fun`;\n",
    "* rebuild the `nodes_list` of the `LightweightGraph`: if the wrapped PyTorch module tree is traversed, the target leaves will have been replaced as specified.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f21494e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "import quantlib.algorithms as qa\n",
    "\n",
    "\n",
    "# nn.Conv2d -> qa.ana.ANAConv2d\n",
    "def replace_conv2d_anaconv2d(module:         nn.Module,\n",
    "                             quantizer_spec: dict,\n",
    "                             noise_type:     str):\n",
    "    \n",
    "    assert isinstance(module, nn.Conv2d)\n",
    "\n",
    "    return qa.ana.ANAConv2d(in_channels=module.in_channels,\n",
    "                            out_channels=module.out_channels,\n",
    "                            kernel_size=module.kernel_size,\n",
    "                            stride=module.stride,\n",
    "                            padding=module.padding,\n",
    "                            dilation=module.dilation,\n",
    "                            groups=module.groups,\n",
    "                            bias=module.bias,\n",
    "                            quantizer_spec=quantizer_spec,\n",
    "                            noise_type=noise_type)\n",
    "\n",
    "\n",
    "class ANAConv2dRule(qlw.rules.LightweightRule):\n",
    "    \n",
    "    def __init__(self,\n",
    "                 filter_:        qlw.rules.filters.Filter,\n",
    "                 quantizer_spec: dict,\n",
    "                 noise_type:     str):\n",
    "\n",
    "        replacement_fun = partial(replace_conv2d_anaconv2d, quantizer_spec=quantizer_spec, noise_type=noise_type)\n",
    "        super(ANAConv2dRule, self).__init__(filter_=filter_, replacement_fun=replacement_fun)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "391792e5",
   "metadata": {},
   "source": [
    "### `LightweightEditor`\n",
    "\n",
    "Sometimes, graph manipulations can be not trivial to implement.\n",
    "This usually happens when the subject network is complicated and multiple steps are involved in the *float2fake* conversion.\n",
    "In these cases, it is more likely to make coding mistakes.\n",
    "Such mistakes can require the user to re-run the network creation code and the *float2fake* conversion, taking a toll in development time.\n",
    "\n",
    "Since Python is an interpreted language, we can perform graph manipulations interactively, possibly undoing those transformations that had an effect other than the intended one.\n",
    "For this purpose, the `quantlib.editing.lighweight` package exposes a `LightweightEditor` abstraction.\n",
    "The usage is the following:\n",
    "\n",
    "* at creation, a `LightweightEditor` wraps a `LightweightGraph`;\n",
    "* the editor protects the wrapped graph from accidental modifications using a lock system called an *editing session*;\n",
    "* to create the lock, call the `startup` method; note that the lock will be created in open position;\n",
    "* to pause an editing session, i.e., to close the lock, call the `pause` method;\n",
    "* to resume an editing session, i.e., to open a previously closed lock, call the `resume` method;\n",
    "* to conclude an editing session, call the `shutdown` method; this call will destroy the lock; note that this action will delete the history of `LightweightRule` applications, making it impossible to undo the applied modifications.\n",
    "\n",
    "When in an editing session, the `LightweightEditor` can set up, apply and unapply modifications:\n",
    "\n",
    "* define the active `LightweightRule` via `set_lwr`; this method takes as argument an object of a class that inherits from `LightweightRule`;\n",
    "* `apply` the active rule; this action has no effect if no rule has been set;\n",
    "* `unapply` the most recently applied rule; this action has no effect if there is no rule application in the history of the editor.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "52ccb5c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "lwe = qlw.LightweightEditor(lwg)\n",
    "lwe.startup()\n",
    "lwe.pause()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6127ddae",
   "metadata": {},
   "source": [
    "Now, we instantiate the previously defined `LightweightRule` to replace all the `torch.nn.Conv2d` modules in the feature extractor with `quantlib.algorithms.ana.ANAConv2d` modules, and apply it to the `LightweightGraph`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5c16f5f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pilot.0                        <class 'quantlib.algorithms.ana.ana_ops.ANAConv2d'>\n",
      "pilot.1                        <class 'torch.nn.modules.batchnorm.BatchNorm2d'>\n",
      "pilot.2                        <class 'torch.nn.modules.activation.ReLU'>\n",
      "features.0                     <class 'torch.nn.modules.pooling.MaxPool2d'>\n",
      "features.1                     <class 'quantlib.algorithms.ana.ana_ops.ANAConv2d'>\n",
      "features.2                     <class 'torch.nn.modules.batchnorm.BatchNorm2d'>\n",
      "features.3                     <class 'torch.nn.modules.activation.ReLU'>\n",
      "features.4                     <class 'quantlib.algorithms.ana.ana_ops.ANAConv2d'>\n",
      "features.5                     <class 'torch.nn.modules.batchnorm.BatchNorm2d'>\n",
      "features.6                     <class 'torch.nn.modules.activation.ReLU'>\n",
      "features.7                     <class 'torch.nn.modules.pooling.MaxPool2d'>\n",
      "features.8                     <class 'quantlib.algorithms.ana.ana_ops.ANAConv2d'>\n",
      "features.9                     <class 'torch.nn.modules.batchnorm.BatchNorm2d'>\n",
      "features.10                    <class 'torch.nn.modules.activation.ReLU'>\n",
      "features.11                    <class 'quantlib.algorithms.ana.ana_ops.ANAConv2d'>\n",
      "features.12                    <class 'torch.nn.modules.batchnorm.BatchNorm2d'>\n",
      "features.13                    <class 'torch.nn.modules.activation.ReLU'>\n",
      "features.14                    <class 'torch.nn.modules.pooling.MaxPool2d'>\n",
      "avgpool                        <class 'torch.nn.modules.pooling.AdaptiveAvgPool2d'>\n",
      "classifier.0                   <class 'torch.nn.modules.linear.Linear'>\n",
      "classifier.1                   <class 'torch.nn.modules.batchnorm.BatchNorm1d'>\n",
      "classifier.2                   <class 'torch.nn.modules.activation.ReLU'>\n",
      "classifier.3                   <class 'torch.nn.modules.linear.Linear'>\n",
      "classifier.4                   <class 'torch.nn.modules.batchnorm.BatchNorm1d'>\n",
      "classifier.5                   <class 'torch.nn.modules.activation.ReLU'>\n",
      "classifier.6                   <class 'torch.nn.modules.linear.Linear'>\n"
     ]
    }
   ],
   "source": [
    "# replace `nn.Conv2d` PyTorch nodes with `qa.ana.ANAConv2d` quantlib nodes\n",
    "rho_conv2d = ANAConv2dRule(filter_conv2d_pilot_or_features, {'nbits': 2, 'signed': True, 'balanced': True, 'eps': 1.0}, 'uniform')\n",
    "\n",
    "lwe.resume()\n",
    "lwe.set_lwr(rho_conv2d)\n",
    "lwe.apply()\n",
    "lwe.pause()\n",
    "\n",
    "lwe.graph.show_nodes_list()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "684b0e8f",
   "metadata": {},
   "source": [
    "### **Exercise**: define custom `LightweightRule`s\n",
    "\n",
    "In the following cells, we will define custom `LightweightRule`s to:\n",
    "\n",
    "* replace all the `torch.nn.Linear` modules in the given list with `quantlib.algorithms.ana.ANALinear` nodes;\n",
    "* replace all the PyTorch modules implementing non-linear transformations (i.e., `torch.nn.ReLU` nodes) with `quantlib.algorithms.ana.ANAActivation`s.\n",
    "\n",
    "In particular, your task is defining the functions that will create fake-quantized ANA modules to replace the given  floating-point PyTorch modules.\n",
    "Remember that the fake-quantized modules should be able to process arrays (i.e., `torch.Tensor`s) with the same shape and with the same data type as those accepted by the floating-point ones.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d59f3241",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nn.Linear -> qa.ana.ANALinear\n",
    "def replace_linear_analinear(module:         torch.nn.Module,\n",
    "                             quantizer_spec: dict,\n",
    "                             noise_type:     str):\n",
    "    \n",
    "    assert isinstance(module, torch.nn.Linear)\n",
    "    \n",
    "    return qa.ana.ANALinear(in_features=module.in_features,\n",
    "                            out_features=module.out_features,\n",
    "                            bias=module.bias,\n",
    "                            quantizer_spec=quantizer_spec,\n",
    "                            noise_type=noise_type)\n",
    "\n",
    "\n",
    "class ANALinearRule(qlw.rules.LightweightRule):\n",
    "    \n",
    "    def __init__(self,\n",
    "                 filter_:        qlw.rules.filters.Filter,\n",
    "                 quantizer_spec: dict,\n",
    "                 noise_type:     str):\n",
    "        \n",
    "        replacement_fun = partial(replace_linear_analinear, quantizer_spec=quantizer_spec, noise_type=noise_type)\n",
    "        super(ANALinearRule, self).__init__(filter_=filter_, replacement_fun=replacement_fun)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e108d684",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nn.ReLU -> qa.ana.ANAActivation\n",
    "def replace_relu_anaact(module:         torch.nn.Module,\n",
    "                        quantizer_spec: dict,\n",
    "                        noise_type:     str):\n",
    "    \n",
    "    assert isinstance(module, torch.nn.ReLU)\n",
    "    return qa.ana.ANAActivation(quantizer_spec=quantizer_spec, noise_type=noise_type)\n",
    "\n",
    "\n",
    "class ANAActivationRule(qlw.rules.LightweightRule):\n",
    "    \n",
    "    def __init__(self,\n",
    "                 filter_:        qlw.rules.filters.Filter,\n",
    "                 quantizer_spec: dict,\n",
    "                 noise_type:     str):\n",
    "        \n",
    "        replacement_fun = partial(replace_relu_anaact, quantizer_spec=quantizer_spec, noise_type=noise_type)\n",
    "        super(ANAActivationRule, self).__init__(filter_=filter_, replacement_fun=replacement_fun)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "011b17a8",
   "metadata": {},
   "source": [
    "### **Exercise**: apply custom `LightweightRule`s\n",
    "\n",
    "In the following cells, we are going to apply the rules that we have just defined to complete the *float2fake* conversion of the VGG8 network.\n",
    "\n",
    "In particular, your task is defining the `Filter`s that will select:\n",
    "\n",
    "* all the `torch.nn.Linear` modules but the last one (leaving the last layer of a QNN at higher precision usually yields much better performance);\n",
    "* all the PyTorch modules implementing ReLU transformations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c62a2a68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pilot.0                        <class 'quantlib.algorithms.ana.ana_ops.ANAConv2d'>\n",
      "pilot.1                        <class 'torch.nn.modules.batchnorm.BatchNorm2d'>\n",
      "pilot.2                        <class 'torch.nn.modules.activation.ReLU'>\n",
      "features.0                     <class 'torch.nn.modules.pooling.MaxPool2d'>\n",
      "features.1                     <class 'quantlib.algorithms.ana.ana_ops.ANAConv2d'>\n",
      "features.2                     <class 'torch.nn.modules.batchnorm.BatchNorm2d'>\n",
      "features.3                     <class 'torch.nn.modules.activation.ReLU'>\n",
      "features.4                     <class 'quantlib.algorithms.ana.ana_ops.ANAConv2d'>\n",
      "features.5                     <class 'torch.nn.modules.batchnorm.BatchNorm2d'>\n",
      "features.6                     <class 'torch.nn.modules.activation.ReLU'>\n",
      "features.7                     <class 'torch.nn.modules.pooling.MaxPool2d'>\n",
      "features.8                     <class 'quantlib.algorithms.ana.ana_ops.ANAConv2d'>\n",
      "features.9                     <class 'torch.nn.modules.batchnorm.BatchNorm2d'>\n",
      "features.10                    <class 'torch.nn.modules.activation.ReLU'>\n",
      "features.11                    <class 'quantlib.algorithms.ana.ana_ops.ANAConv2d'>\n",
      "features.12                    <class 'torch.nn.modules.batchnorm.BatchNorm2d'>\n",
      "features.13                    <class 'torch.nn.modules.activation.ReLU'>\n",
      "features.14                    <class 'torch.nn.modules.pooling.MaxPool2d'>\n",
      "avgpool                        <class 'torch.nn.modules.pooling.AdaptiveAvgPool2d'>\n",
      "classifier.0                   <class 'quantlib.algorithms.ana.ana_ops.ANALinear'>\n",
      "classifier.1                   <class 'torch.nn.modules.batchnorm.BatchNorm1d'>\n",
      "classifier.2                   <class 'torch.nn.modules.activation.ReLU'>\n",
      "classifier.3                   <class 'quantlib.algorithms.ana.ana_ops.ANALinear'>\n",
      "classifier.4                   <class 'torch.nn.modules.batchnorm.BatchNorm1d'>\n",
      "classifier.5                   <class 'torch.nn.modules.activation.ReLU'>\n",
      "classifier.6                   <class 'torch.nn.modules.linear.Linear'>\n"
     ]
    }
   ],
   "source": [
    "# replace `nn.Linear` PyTorch nodes with `qa.ana.ANALinear` quantlib nodes\n",
    "filter_last_layer = qlw.rules.filters.NameFilter('classifier.6')\n",
    "filter_linear_classifier_no_last = filter_linear & filter_classifier & (-filter_last_layer)\n",
    "rho_linear = ANALinearRule(filter_linear_classifier_no_last, {'nbits': 2, 'signed': True, 'balanced': True, 'eps': 1.0}, 'uniform')\n",
    "\n",
    "lwe.resume()\n",
    "lwe.set_lwr(rho_linear)\n",
    "lwe.apply()\n",
    "lwe.pause()\n",
    "\n",
    "lwe.graph.show_nodes_list()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8a2fcf08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pilot.0                        <class 'quantlib.algorithms.ana.ana_ops.ANAConv2d'>\n",
      "pilot.1                        <class 'torch.nn.modules.batchnorm.BatchNorm2d'>\n",
      "pilot.2                        <class 'quantlib.algorithms.ana.ana_ops.ANAActivation'>\n",
      "features.0                     <class 'torch.nn.modules.pooling.MaxPool2d'>\n",
      "features.1                     <class 'quantlib.algorithms.ana.ana_ops.ANAConv2d'>\n",
      "features.2                     <class 'torch.nn.modules.batchnorm.BatchNorm2d'>\n",
      "features.3                     <class 'quantlib.algorithms.ana.ana_ops.ANAActivation'>\n",
      "features.4                     <class 'quantlib.algorithms.ana.ana_ops.ANAConv2d'>\n",
      "features.5                     <class 'torch.nn.modules.batchnorm.BatchNorm2d'>\n",
      "features.6                     <class 'quantlib.algorithms.ana.ana_ops.ANAActivation'>\n",
      "features.7                     <class 'torch.nn.modules.pooling.MaxPool2d'>\n",
      "features.8                     <class 'quantlib.algorithms.ana.ana_ops.ANAConv2d'>\n",
      "features.9                     <class 'torch.nn.modules.batchnorm.BatchNorm2d'>\n",
      "features.10                    <class 'quantlib.algorithms.ana.ana_ops.ANAActivation'>\n",
      "features.11                    <class 'quantlib.algorithms.ana.ana_ops.ANAConv2d'>\n",
      "features.12                    <class 'torch.nn.modules.batchnorm.BatchNorm2d'>\n",
      "features.13                    <class 'quantlib.algorithms.ana.ana_ops.ANAActivation'>\n",
      "features.14                    <class 'torch.nn.modules.pooling.MaxPool2d'>\n",
      "avgpool                        <class 'torch.nn.modules.pooling.AdaptiveAvgPool2d'>\n",
      "classifier.0                   <class 'quantlib.algorithms.ana.ana_ops.ANALinear'>\n",
      "classifier.1                   <class 'torch.nn.modules.batchnorm.BatchNorm1d'>\n",
      "classifier.2                   <class 'quantlib.algorithms.ana.ana_ops.ANAActivation'>\n",
      "classifier.3                   <class 'quantlib.algorithms.ana.ana_ops.ANALinear'>\n",
      "classifier.4                   <class 'torch.nn.modules.batchnorm.BatchNorm1d'>\n",
      "classifier.5                   <class 'quantlib.algorithms.ana.ana_ops.ANAActivation'>\n",
      "classifier.6                   <class 'torch.nn.modules.linear.Linear'>\n"
     ]
    }
   ],
   "source": [
    "# replace `nn.ReLU` PyTorch nodes with `qa.ana.ANAActivation` quantlib nodes\n",
    "filter_relu_pilot_or_features_or_classifier = filter_relu & (filter_pilot | filter_features | filter_classifier)\n",
    "rho_relu = ANAActivationRule(filter_relu_pilot_or_features_or_classifier, {'nbits': 2, 'signed': True, 'balanced': True, 'eps': 1.0}, 'uniform')\n",
    "\n",
    "lwe.resume()\n",
    "lwe.set_lwr(rho_relu)\n",
    "lwe.apply()\n",
    "lwe.pause()\n",
    "\n",
    "lwe.graph.show_nodes_list()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "677d8adc",
   "metadata": {},
   "source": [
    "When we are happy with the transformation applied to the graph (i.e., when we are sure that we replaced the intended modules with the quantized counterparts), we can close the editing session.\n",
    "Recall that this operation will destroy the editing history, and it will be impossible to bring the network back into the state it was in before the editing process.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5382195f",
   "metadata": {},
   "outputs": [],
   "source": [
    "lwe.shutdown()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed92ef96",
   "metadata": {},
   "source": [
    "## Intermezzo: training a fake-quantized QNN using the ANA algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6d162ee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "try:\n",
    "    ckpt_path = os.path.join(os.pardir, os.pardir, os.pardir, 'systems', 'CIFAR10', 'VGG', 'logs', 'exp0000', 'fold0', 'saves', 'best.ckpt')\n",
    "    ckpt      = torch.load(ckpt_path, map_location=torch.device('cpu'))\n",
    "    net.load_state_dict(ckpt['net'])\n",
    "except OSError:\n",
    "    for anamod in filter(lambda m: isinstance(m, qa.ana.ana_ops.ANAModule), net.modules()):\n",
    "        anamod.set_fnoise(0.0, 0.0)\n",
    "        anamod.set_bnoise(0.0, 0.0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7189cacb",
   "metadata": {},
   "source": [
    "## Converting a fake-quantized graph into a true-quantized one\n",
    "\n",
    "At this point, we assume that we have been able to train the ANA-fake-quantized VGG8 network.\n",
    "It is therefore time to proceed with deriving the software \"golden model\" for the program that will actually run on CUTIE, i.e., the CUTIE-compatible, true-quantized VGG8.\n",
    "\n",
    "This process will require several steps:\n",
    "\n",
    "* *tracing* the fake-quantized network object into a PyTorch *just-in-time* (JIT) computational graph;\n",
    "* *elevating* a JIT graph to a PyTorch graph;\n",
    "* inserting *helper* operations;\n",
    "* *rewriting* PyTorch graphs;\n",
    "* removing *helper* operations;\n",
    "* *generating code* for a true-quantized network object.\n",
    "\n",
    "We start by importing the namespaces providing the general-purpose and backend-specific *fake2true* conversion functionalities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e08aeae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import quantlib.editing.graphs as qg\n",
    "import quantlib.backends as qb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0be2e7b8",
   "metadata": {},
   "source": [
    "Since for didactic purposes it can be useful to visualise the graphs involved in a *fake2true* conversion, we define the following helper function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b678bc69",
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "from IPython.display import display, IFrame\n",
    "\n",
    "\n",
    "def print_and_render(G:        nx.DiGraph,\n",
    "                     filename: str,\n",
    "                     width:    int = 950,\n",
    "                     height:   int = 600) -> None:\n",
    "    \n",
    "    dir_figures = os.path.join(os.path.curdir, 'figures')\n",
    "    if not os.path.isdir(dir_figures):\n",
    "        os.makedirs(dir_figures, exist_ok=True)\n",
    "\n",
    "    qg.utils.draw_graph(G, dir_figures, filename)\n",
    "    display(IFrame(os.path.join(dir_figures, filename + '.pdf'), width, height))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0527f9f9",
   "metadata": {},
   "source": [
    "### *Tracing* a PyTorch network object into a JIT graph: `ONNXGraph`s\n",
    "\n",
    "In PyTorch jargon, *tracing* is the process by which a mock-up array (also called *dummy input*) is given in input to a PyTorch network and the calls triggered during the construction of the dynamic computational graph are registered.\n",
    "This allows to retrieve the full dependency structure between arrays and functions.\n",
    "\n",
    "Before tracing a fake-quantized graph, be sure to freeze its data-dependent parameters to their data-independent versions (e.g., the channel means and standard deviations tracked by batch normalisation modules).\n",
    "At runtime it is generally preferable **not** using data-dependent parameters, since this constraint makes it easier to conceive optimisations.\n",
    "\n",
    "In the specific case of networks that include batch normalisation layers, it is impossible to trace such networks by passing batches that include a single data point when the networks are not set in \"evaluation\" mode (by definition, computing the sample standard deviation associated with a sample that contains less than two data points is impossible).\n",
    "Therefore, we suggest to trace networks setting the batch size of the dummy input to one.\n",
    "\n",
    "In `quantlib`, the tracing operation is performed implicitly by wrapping the PyTorch network object inside a `quantlib.editing.graphs.graphs.ONNXGraph` object.\n",
    "The constructor method of this class takes in input an object of class `torch.nn.Module` and a `torch.Tensor` representing a dummy input (which shape should be compatible to that expected in input by the network), and automatically performs the tracing.\n",
    "The graph resulting from the tracing is stored in an `nx_graph` attribute, an object of class `networkx.DiGraph`.\n",
    "\n",
    "We must point out that the class name we chose for this graph is a bit misleading: despite the fact that PyTorch JIT graph are tightly related to the data structures specified by the ONNX format, PyTorch JIT graphs register more information that the one specified by the standard ONNX specification.\n",
    "Beware of this fact, and don't make any assumption about the fact that objects of type `ONNXGraph` are directly compatible with ONNX specifications.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8de0e400",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/spmatteo/anaconda3/envs/quantlab/lib/python3.8/site-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /Users/distiller/project/conda/conda-bld/pytorch_1623459044803/work/c10/core/TensorImpl.h:1156.)\n",
      "  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n",
      "/Users/spmatteo/anaconda3/envs/quantlab/lib/python3.8/site-packages/torch/onnx/symbolic_helper.py:712: UserWarning: ONNX export mode is set to inference mode, but operator batch_norm is set to inference mode. The model will be exported in inference, as specified by the export mode.\n",
      "  warnings.warn(\"ONNX export mode is set to \" + training_mode +\n"
     ]
    }
   ],
   "source": [
    "net.eval()\n",
    "onnxg = qg.graphs.ONNXGraph(net, torch.randn((1, 3, 32, 32)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dc378b1",
   "metadata": {},
   "source": [
    "### *Elevating* a JIT graph to a PyTorch graph\n",
    "\n",
    "Since PyTorch JIT graphs keep track of all the arrays and low-level kernels utilised by a PyTorch network object, they often consist of hundreds or even thousands of nodes.\n",
    "\n",
    "Due to large variety of computational graphs arising from tracing different PyTorch network objects, the patterns involved in the *fake2true* conversions have not yet been categorized.\n",
    "Consequently, the application of sound compiler-theoretical tools has not yet been enabled.\n",
    "Or at least, this is the current state of research in the field to the best of our knowledge.\n",
    "\n",
    "Writing proper graph rewriting rules for such graphs might be daunting and error prone.\n",
    "For this reason, we introduced the concept of *elevation* of a PyTorch JIT graph into a PyTorch graph.\n",
    "A PyTorch graph is the computational graph representation of a PyTorch network object, obtained by making the dependencies between its basic `torch.nn.Module`s explicit.\n",
    "This transformation makes it easier to write GRRs for *fake2true* quantization, hopefully reducing the probability of making mistakes.\n",
    "\n",
    "This capability comes at the price of automatically deriving the connectivity of the PyTorch graph by analising the corresponding JIT graph.\n",
    "We started by observing that most operations in a JIT graph are executed in the scope of some PyTorch `torch.nn.Module` operation.\n",
    "However, some operations are executed under the scope of direct calls to the API of `torch.Tensor` objects (e.g., `torch.view` is used to \"linearise\" the multi-dimensional arrays output by convolutional modules before feeding them to linear modules); such operations remain unscoped after the tracing process.\n",
    "Finally, some custom fake-quantized PyTorch modules might use operations that are not properly scoped during the tracing step.\n",
    "\n",
    "Performing an *elevation* automatically requires that **all** the kernel nodes in an `ONNXGraph` are properly scoped.\n",
    "The `quantlib.editing.graphs` package provides some functionalities that can be used to identify and re-scope such operations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b8e43089",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"950\"\n",
       "            height=\"600\"\n",
       "            src=\"./figures/VGG8_ANA_F2T_ONNX.pdf\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7fee5e450d00>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "onnxe = qg.editor.Editor(onnxg)\n",
    "\n",
    "print_and_render(onnxe.G, 'VGG8_ANA_F2T_ONNX')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b13b218f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "O000039\n",
      "O000040\n",
      "O000043\n",
      "O000042\n",
      "O000044\n",
      "O000041\n"
     ]
    }
   ],
   "source": [
    "for node_id, scope in nx.get_node_attributes(onnxe.G, 'scope').items():\n",
    "    if scope == '':\n",
    "        print(node_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fbb0f387",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Temporary cache directory created at /var/folders/6h/rm9g6bs143g4mvwfh3fd7_jw0000gq/T/tmpl32ms551\n",
      "Applying rule <class 'quantlib.editing.graphs.grrules.dporules.AutoRescopingRule'> to `nn.Module`s of type AdaptiveAvgPool2d ...\n",
      "Applying rule <class 'quantlib.editing.graphs.grrules.dporules.ManualRescopingRule'> to `nn.Module`s of type ViewFlattenNd ...\n",
      "Applying rule <class 'quantlib.editing.graphs.grrules.dporules.AutoRescopingRule'> to `nn.Module`s of type ANAActivation ...\n",
      "Applying rule <class 'quantlib.editing.graphs.grrules.dporules.AutoRescopingRule'> to `nn.Module`s of type ANAConv2d ...\n",
      "An issue arose while applying rule <class 'quantlib.editing.graphs.grrules.dporules.AutoRescopingRule'> to graph <> at point: \n",
      "\t O000013 L-term/O000000\n",
      "\t D000052 L-term/D000007\n",
      "\t O000014 L-term/O000001\n",
      "\t D000053 L-term/D000009\n",
      "\t O000017 L-term/O000002\n",
      "\t D000064 K-term/D000011\n",
      "\t D000048 K-term/D000000\n",
      "\t D000047 K-term/D000003\n",
      "\t D000049 K-term/D000004\n",
      "\t D000051 K-term/D000005\n",
      "\t D000044 K-term/D000008\n",
      "\t D000050 K-term/D000002\n",
      "\t D000045 K-term/D000001\n",
      "\t D000046 K-term/D000006\n",
      "\t D000063 K-term/D000010\n",
      "\n",
      "An issue arose while applying rule <class 'quantlib.editing.graphs.grrules.dporules.AutoRescopingRule'> to graph <> at point: \n",
      "\t O000028 L-term/O000000\n",
      "\t D000105 L-term/D000007\n",
      "\t O000029 L-term/O000001\n",
      "\t D000106 L-term/D000009\n",
      "\t O000032 L-term/O000002\n",
      "\t D000117 K-term/D000011\n",
      "\t D000098 K-term/D000000\n",
      "\t D000103 K-term/D000003\n",
      "\t D000102 K-term/D000004\n",
      "\t D000101 K-term/D000005\n",
      "\t D000097 K-term/D000008\n",
      "\t D000104 K-term/D000002\n",
      "\t D000100 K-term/D000001\n",
      "\t D000099 K-term/D000006\n",
      "\t D000116 K-term/D000010\n",
      "\n",
      "Applying rule <class 'quantlib.editing.graphs.grrules.dporules.AutoRescopingRule'> to `nn.Module`s of type ANALinear ...\n"
     ]
    }
   ],
   "source": [
    "onnxe.startup()\n",
    "for mod_name, rho in qg.grrules.load_rescoping_rules(modules=['AdaptiveAvgPool2d', 'ViewFlattenNd', 'ANAActivation', 'ANAConv2d', 'ANALinear']).items():\n",
    "    print(\"Applying rule {} to `nn.Module`s of type {} ...\".format(type(rho), mod_name))\n",
    "    onnxe.set_grr(rho)\n",
    "    onnxe.edit()\n",
    "onnxe.shutdown()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a24633be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "O000039\n",
      "O000040\n",
      "O000043\n",
      "O000042\n",
      "O000044\n",
      "O000041\n"
     ]
    }
   ],
   "source": [
    "for node_id, scope in nx.get_node_attributes(onnxe.G, 'scope').items():\n",
    "    if scope == '':\n",
    "        print(node_id)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f2ec14f",
   "metadata": {},
   "source": [
    "After all nodes have been scoped, a `PyTorchGraph` object can be used to create a PyTorch graph.\n",
    "Its constructor method takes in input the PyTorch network object that was fed to the tracing function (i.e., to the constructor method of the `ONNXGraph` object itself) and the `ONNXGraph` object.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c1db0f04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"950\"\n",
       "            height=\"600\"\n",
       "            src=\"./figures/VGG8_ANA_F2T_PyTorch.pdf\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7fee8070e8b0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pytorchg = qg.graphs.PyTorchGraph(net, onnxg)\n",
    "pytorche = qg.editor.Editor(pytorchg, onlykernel=True)\n",
    "\n",
    "print_and_render(pytorche.G, 'VGG8_ANA_F2T_PyTorch')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c6ad246",
   "metadata": {},
   "source": [
    "### Adding *helper* nodes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6d328922",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Temporary cache directory created at /var/folders/6h/rm9g6bs143g4mvwfh3fd7_jw0000gq/T/tmp5psofwod\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"950\"\n",
       "            height=\"600\"\n",
       "            src=\"./figures/VGG8_ANA_F2T_preproc_IO.pdf\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7fee8070e820>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pytorche.add_io_handles()\n",
    "\n",
    "print_and_render(pytorche.G, 'VGG8_ANA_F2T_preproc_IO')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9ab242e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Temporary cache directory created at /var/folders/6h/rm9g6bs143g4mvwfh3fd7_jw0000gq/T/tmpe27g7i64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"950\"\n",
       "            height=\"600\"\n",
       "            src=\"./figures/VGG8_ANA_F2T_preproc_PT.pdf\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7fee879fd5b0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pytorche.startup()\n",
    "\n",
    "# apply the idempotence property to create a graph that is decomposable in non-intersecting translation units\n",
    "pytorche.resume()\n",
    "pytorche.set_grr(qg.grrules.AddPrecisionTunnelRule(qa.ana.ANAActivation.__name__))\n",
    "pytorche.edit()\n",
    "pytorche.pause()\n",
    "\n",
    "print_and_render(pytorche.G, 'VGG8_ANA_F2T_preproc_PT')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60765d23",
   "metadata": {},
   "source": [
    "Since the goal of *fake2true* conversions is generating software \"golden models\" of the programs that will run on the target platform, and since we decide to operate with PyTorch modules, we must have access to PyTorch abstractions that mimic such operations.\n",
    "\n",
    "The CUTIE accelerator re-ternarizes the integer outputs of ternary convolutions by two-way thresholding - we model this behavior with the `LUTActivation` class, which takes an array of sets of `k` monotonically increasing thresholds (`tau`) and a single set of `k+1` quantization levels to which values are mapped.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2a1c88be",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LUTActivation(nn.Module):\n",
    "    \n",
    "    def __init__(self, tau, quant_levels):\n",
    "        super(LUTActivation, self).__init__()\n",
    "        self.setup_parameters(self, tau, quant_levels)\n",
    "        self.unfolded_tau = False\n",
    "    \n",
    "    @staticmethod\n",
    "    def setup_parameters(lutmod, tau, quant_levels):\n",
    "        lutmod.register_parameter('tau', nn.Parameter(tau, requires_grad=False))\n",
    "        lutmod.register_parameter('quant_levels', nn.Parameter(quant_levels, requires_grad=False))\n",
    "        lutmod.register_parameter('q0', nn.Parameter(quant_levels[0], requires_grad=False))\n",
    "        lutmod.register_parameter('jumps', nn.Parameter(quant_levels[1:] - quant_levels[:-1], requires_grad=False))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        if not self.unfolded_tau:\n",
    "            self.tau.data     = self.tau[(...,) + (None,) * (x.dim() - 2)]\n",
    "            self.unfolded_tau = True\n",
    "            \n",
    "        x = x.unsqueeze(1)\n",
    "        cdf = (x - self.tau >= 0.0).float()\n",
    "        \n",
    "        y = self.q0 + torch.sum(self.jumps[(...,) + (None,) * (cdf.dim() - 2)] * cdf, 1)\n",
    "\n",
    "        return y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edfff5f6",
   "metadata": {},
   "source": [
    "We have finally reached the point where we can write our first full graph rewriting rule involving some arithmetical conversions!\n",
    "In the next cells, we will define the aritmetic conversion function and the double push-out (DPO) GRR that uses such conversion.\n",
    "\n",
    "DPO GRRs should be defined by sub-classing the `DPORule` class exposed by the general-purpose module `quantlib/editing/graph/grrules/dporules.py`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ef560819",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def fold_anaact_anaconv2d_bn2d_anaact(eps_x, eps_w, weight, mi, sigma, bn_eps, gamma, beta, eps_s, theta,\n",
    "                                      ceiltau=True):\n",
    "\n",
    "    def torch2numpyfp64(x):\n",
    "        return x.detach().cpu().numpy().astype(np.float64)\n",
    "\n",
    "    eps_x  = torch2numpyfp64(eps_x)\n",
    "    eps_w  = torch2numpyfp64(eps_w)\n",
    "    weight = torch2numpyfp64(weight)\n",
    "    mi     = torch2numpyfp64(mi)\n",
    "    sigma  = torch2numpyfp64(sigma)\n",
    "    gamma  = torch2numpyfp64(gamma)\n",
    "    beta   = torch2numpyfp64(beta)\n",
    "    eps_s  = torch2numpyfp64(eps_s)\n",
    "    theta  = torch2numpyfp64(theta)\n",
    "    \n",
    "    # compensate for negative gammas\n",
    "    flip   = np.sign(gamma)\n",
    "    w_tmp  = weight.transpose(1, 2, 3, 0)\n",
    "    w_tmp *= flip\n",
    "    weight = w_tmp.transpose(3, 0, 1, 2)\n",
    "\n",
    "    # https://github.com/pytorch/pytorch/blob/b5e832111e5e4bb3dd66d716d398b81fe70c6af0/torch/csrc/jit/tensorexpr/kernel.cpp#L2015\n",
    "    sigma = np.sqrt(sigma + bn_eps)\n",
    "    \n",
    "    # folding\n",
    "    xi   = gamma * (sigma ** -1)\n",
    "    zeta = beta - mi * xi\n",
    "    \n",
    "    gammaprime = flip * (xi * (eps_x * eps_w) / eps_s) # * (eps_s ** -1)\n",
    "    betaprime  = zeta / eps_s # * (eps_s ** -1)\n",
    "\n",
    "    # prepare for broadcasting\n",
    "    gammaprime = np.expand_dims(gammaprime, axis=0)\n",
    "    betaprime  = np.expand_dims(betaprime, axis=0)\n",
    "    theta      = np.expand_dims(theta, axis=-1)\n",
    "    \n",
    "    # absorb folded parameters into thresholds\n",
    "    tau = (theta - betaprime) / gammaprime # * (gammaprime ** -1)\n",
    "        \n",
    "    assert np.all(tau[0] < tau[1])\n",
    "    \n",
    "    def numpy2torchfp64(x):\n",
    "        return torch.from_numpy(x.astype(np.float64))\n",
    "\n",
    "    if ceiltau:\n",
    "        return numpy2torchfp64(np.ceil(tau)).float(), numpy2torchfp64(weight).float()\n",
    "    else:\n",
    "        return numpy2torchfp64(tau).float(), numpy2torchfp64(weight).float()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "efd00c53",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "import itertools\n",
    "\n",
    "from quantlib.editing.graphs.grrules.dporules import DPORule\n",
    "from quantlib.editing.graphs.graphs import Bipartite\n",
    "from quantlib.editing.graphs.grrules import Seeker\n",
    "from quantlib.editing.graphs.graphs.nodes import __NODE_ID_FORMAT__, PyTorchNode\n",
    "\n",
    "\n",
    "class FoldANAActANAConvBNANAActTypeARule(DPORule):  # w/o max pooling\n",
    "\n",
    "    def __init__(self, lut_entry_bits=16):\n",
    "\n",
    "        self._lut_entry_bits = lut_entry_bits\n",
    "\n",
    "        # Nodes of the interface\n",
    "        K_types = OrderedDict()\n",
    "        K_types.update({'HPTout': qg.graphs.HelperOutputPrecisionTunnel.__name__})\n",
    "        K_types.update({'HPTin':  qg.graphs.HelperInputPrecisionTunnel.__name__})\n",
    "        K_types = OrderedDict([('/'.join(['K-term', k]), v) for k, v in K_types.items()])\n",
    "\n",
    "        # Nodes in the core template graph\n",
    "        LK_types = OrderedDict()\n",
    "        LK_types.update({'ANAActin':  qa.ana.ANAActivation.__name__})\n",
    "        LK_types.update({'ANAConv':   qa.ana.ANAConv2d.__name__})\n",
    "        LK_types.update({'BatchNorm': nn.BatchNorm2d.__name__})\n",
    "        LK_types.update({'ANAActout': qa.ana.ANAActivation.__name__})\n",
    "        LK_types = OrderedDict([('/'.join(['L-term', k]), v) for k, v in LK_types.items()])\n",
    "\n",
    "        # Nodes in the core replacement graph\n",
    "        RK_types = OrderedDict()\n",
    "        RK_types.update({'TWConv': nn.Conv2d.__name__})\n",
    "        RK_types.update({'LUTAct': LUTActivation.__name__})\n",
    "        RK_types = OrderedDict([('/'.join(['R-term', k]), v) for k, v in RK_types.items()])\n",
    "\n",
    "        K_node_IDs  = list(K_types.keys())\n",
    "        LK_node_IDs = list(LK_types.keys())\n",
    "        RK_node_IDs = list(RK_types.keys())\n",
    "\n",
    "        # define the template graph L [L-term]\n",
    "        L_node_IDs = [K_node_IDs[0]] + LK_node_IDs + [K_node_IDs[-1]]\n",
    "        self.L = nx.DiGraph()\n",
    "        # Define arcs between nodes in full template graph\n",
    "        self.L.add_edges_from({(u, v) for u, v in zip(L_node_IDs[:-1], L_node_IDs[1:])})\n",
    "\n",
    "        # Here, graph is only operation nodes\n",
    "        # Necessary for seeker\n",
    "        nx.set_node_attributes(self.L, {vL: Bipartite.KERNEL for vL in set(self.L.nodes)}, 'bipartite')\n",
    "        nx.set_node_attributes(self.L, {**K_types, **LK_types}, 'type')\n",
    "\n",
    "        # define the context (sub-)graph K [K-term]\n",
    "        VK = set(K_node_IDs)  # precision tunnel nodes define the context graph\n",
    "        self.K = self.L.subgraph(VK)\n",
    "\n",
    "        # define the template (sub-)graph L\\K\n",
    "        VLK = set(self.L.nodes).difference(set(self.K.nodes))\n",
    "        self.LK = self.L.subgraph(VLK)\n",
    "\n",
    "        # define the replacement (sub-)graph R\\K [\"gluing\" R\\K to K yields the graph R, i.e., the R-term]\n",
    "        self.RK = nx.DiGraph()\n",
    "        self.RK.add_edges_from({(u, v) for u, v in zip(RK_node_IDs[:-1], RK_node_IDs[1:])})\n",
    "        nx.set_node_attributes(self.RK, {vRK: Bipartite.KERNEL for vRK in set(self.RK.nodes)}, 'bipartite')\n",
    "        nx.set_node_attributes(self.RK, RK_types, 'type')\n",
    "\n",
    "        # define the arcs that go from the vertices of K to those of R\\K, and viceversa\n",
    "        E_K2RK = {(K_node_IDs[0], RK_node_IDs[0])}\n",
    "        E_RK2K = {(RK_node_IDs[-1], K_node_IDs[-1])}\n",
    "        E_K2RK2K = E_K2RK | E_RK2K\n",
    "        # disintegrate `E_K2RK` and `E_RK2K` along fibres to speed up rule application\n",
    "        # A fibre is kind of like fixing one argument of a two input one output function and looking at all possible outputs\n",
    "        self.F_K2RK = {vK: set(arc for arc in E_K2RK if arc[0] == vK) for vK in set(self.K.nodes)}\n",
    "        self.F_RK2K = {vK: set(arc for arc in E_RK2K if arc[1] == vK) for vK in set(self.K.nodes)}\n",
    "\n",
    "        # # glue together the (sub-)graphs L\\K and R\\K along the vertices of K\n",
    "        # self.S = nx.compose(self.L, self.RK)\n",
    "        # self.S.add_edges_from(E_K2RK2K)\n",
    "\n",
    "        # since the GRR's L-term has been modified, rebuild the seeker\n",
    "        self.seeker = Seeker(self.L)\n",
    "\n",
    "        # this machinery can generate always-new identifiers for different rule applications\n",
    "        self._counter = itertools.count()\n",
    "\n",
    "    def _get_rule_count(self):\n",
    "        rule_count = ''.join(['FANABNANATA', __NODE_ID_FORMAT__.format(next(self._counter))])\n",
    "        return rule_count\n",
    "\n",
    "    def core(self, HI, g, nodes_dict):\n",
    "\n",
    "        # generate the substitute (sub-)graph J\\I\n",
    "        rule_count = self._get_rule_count()\n",
    "        g_RK2JI = {vRK: '_'.join([rule_count, vRK.replace('R-term/', '')]) for vRK in set(self.RK.nodes)}\n",
    "        JI = nx.relabel_nodes(self.RK, g_RK2JI, copy=True)\n",
    "\n",
    "        # get pointers to the old modules;\n",
    "        # these pointers will enable two actions:\n",
    "        #   1. extracting the arguments required to perform the folding\n",
    "        #   2. extracting the parameters to instantiate the new modules\n",
    "        g_L2H = {vL: vH for vH, vL in g.items()}\n",
    "        manain  = nodes_dict[g_L2H['/'.join(['L-term', 'ANAActin'])]].nobj\n",
    "        mconv2d = nodes_dict[g_L2H['/'.join(['L-term', 'ANAConv'])]].nobj\n",
    "        mbn2d   = nodes_dict[g_L2H['/'.join(['L-term', 'BatchNorm'])]].nobj\n",
    "        manaout = nodes_dict[g_L2H['/'.join(['L-term', 'ANAActout'])]].nobj\n",
    "\n",
    "        # fold\n",
    "        tau, weight = fold_anaact_anaconv2d_bn2d_anaact(manain.eps,\n",
    "                                                        mconv2d.eps, mconv2d.weight_maybe_quant,\n",
    "                                                        mbn2d.running_mean, mbn2d.running_var, mbn2d.eps, mbn2d.weight, mbn2d.bias,\n",
    "                                                        manaout.eps,\n",
    "                                                        manaout.thresholds)\n",
    "        \n",
    "        # build the new modules\n",
    "        mtwconv = nn.Conv2d(mconv2d.in_channels, mconv2d.out_channels, mconv2d.kernel_size,\n",
    "                            stride=mconv2d.stride, padding=mconv2d.padding, dilation=mconv2d.dilation, groups=mconv2d.groups,\n",
    "                            bias=mconv2d.bias is not None).to(torch.device('cpu'))\n",
    "        mtwconv.weight.data = weight\n",
    "\n",
    "        mlutact = LUTActivation(tau, manaout.quant_levels)\n",
    "\n",
    "        # register the newly created nodes\n",
    "        vJI_2_ptnode = {}\n",
    "        vJI_2_ptnode[g_RK2JI['/'.join(['R-term', 'TWConv'])]] = PyTorchNode(mtwconv)\n",
    "        vJI_2_ptnode[g_RK2JI['/'.join(['R-term', 'LUTAct'])]] = PyTorchNode(mlutact)\n",
    "\n",
    "        return JI, vJI_2_ptnode\n",
    "\n",
    "    # G: Full/original graph\n",
    "    # nodes_dict: Mapping between node identifiers of G and actual underlying objects\n",
    "    # g: One instance of all occurences of the template in G, i.e. one application point for the replacement rule -> one morphism\n",
    "    def apply(self, G, nodes_dict, g):\n",
    "\n",
    "        # create new containers\n",
    "        G = G.copy()\n",
    "        # Dictionary mapping of node identifiers to a payload\n",
    "        # keys in nodes_dict should be the same as G.nodes\n",
    "        nodes_dict = {**nodes_dict}\n",
    "\n",
    "        # characterise the match graph H\n",
    "        # Occurence of template in the graph\n",
    "        # SPMATTEO: Some assumptions to discuss\n",
    "        VI = {vH for vH, vL in g.items() if vL in set(self.K.nodes)} # Occurence of context\n",
    "        VHI = {vH for vH, vL in g.items() if vL not in set(self.K.nodes)} # Occurence of core template\n",
    "        HI = G.subgraph(VHI) # HI is the subgraph induced by the set of nodes VHI\n",
    "\n",
    "        # generate the substitute (sub-)graph J\\I (completely detached from G)\n",
    "        # Instantiate blueprint of the replacement graph\n",
    "        JI, vJI_2_ptnode = self.core(HI, g, nodes_dict)\n",
    "\n",
    "        # add the substitute (sub-)graph J\\I to the main graph G\n",
    "        G = nx.compose(G, JI) # G now has two connected but 'independent' subgraphs\n",
    "        nodes_dict.update(vJI_2_ptnode) # Add new payloads from substitute graph\n",
    "\n",
    "        # glue the substitute (sub-)graph J\\I to the interface (sub-)graph I\n",
    "        JI2RK_morphisms = Seeker(self.RK).get_morphisms(JI)\n",
    "        assert len(JI2RK_morphisms) == 1\n",
    "        g_JI2RK = JI2RK_morphisms[0]\n",
    "        g_RK2JI = {vRK: vJI for vJI, vRK in g_JI2RK.items()}\n",
    "        for vI in VI: # for each node in the interface subgraph of G\n",
    "            vK = g[vI]\n",
    "            G.add_edges_from({(vI, g_RK2JI[vRK]) for (_, vRK) in self.F_K2RK[vK]}) # incoming interface connections from G to substitute graph\n",
    "            G.add_edges_from({(g_RK2JI[vRK], vI) for (vRK, _) in self.F_RK2K[vK]}) # outcoming interface connections from substitute graph to G\n",
    "            # the new modules are fully integerized, so the precision tunnel should not embed integer numbers in floating point numbers\n",
    "            # Specific to integer arithmetic transformation -> No relation to graph editing, per-se\n",
    "            if nodes_dict[vI].ntype == qg.graphs.HelperOutputPrecisionTunnel.__name__:\n",
    "                nodes_dict[vI] = PyTorchNode(qg.graphs.HelperOutputPrecisionTunnel(1.0))\n",
    "            elif nodes_dict[vI].ntype == qg.graphs.HelperInputPrecisionTunnel.__name__:\n",
    "                nodes_dict[vI] = PyTorchNode(qg.graphs.HelperInputPrecisionTunnel(1.0))\n",
    "            else:\n",
    "                raise TypeError  # interface nodes should be objects of class `qg.graphs.HelperPrecisionTunnel` only\n",
    "\n",
    "        # discard the match (sub-)graph H\\I\n",
    "        # Assumption: removing a node also removes all arcs pointing to or from that node\n",
    "        G.remove_nodes_from(set(HI.nodes))\n",
    "\n",
    "        # Remove the payload, i.e. underying objects, accordingly\n",
    "        for vHI in VHI:\n",
    "            del nodes_dict[vHI]\n",
    "\n",
    "        return G, nodes_dict\n",
    "\n",
    "    def seek(self, G, nodes_dict):\n",
    "        gs = self.seeker.get_morphisms(G)\n",
    "        return gs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "fe15b37e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"950\"\n",
       "            height=\"600\"\n",
       "            src=\"./figures/VGG8_ANA_F2T_derivation.pdf\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7fee88be6d00>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pytorche.resume()\n",
    "pytorche.set_grr(FoldANAActANAConvBNANAActTypeARule())\n",
    "pytorche.edit()\n",
    "pytorche.pause()\n",
    "\n",
    "print_and_render(pytorche.G, 'VGG8_ANA_F2T_derivation')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c3d0e74f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from networkx.algorithms.dag import topological_sort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4ba7ad59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HI000000 \t\t HelperInput\n",
      "O000000 \t\t ANAConv2d\n",
      "O000001 \t\t BatchNorm2d\n",
      "O000002 \t\t ANAActivation\n",
      "HPTin000002 \t\t HelperInputPrecisionTunnel\n",
      "HPTout000003 \t\t HelperOutputPrecisionTunnel\n",
      "HPTclone000003 \t\t ANAActivation\n",
      "O000003 \t\t MaxPool2d\n",
      "O000004 \t\t ANAConv2d\n",
      "O000005 \t\t BatchNorm2d\n",
      "O000006 \t\t ANAActivation\n",
      "HPTin000006 \t\t HelperInputPrecisionTunnel\n",
      "HPTout000007 \t\t HelperOutputPrecisionTunnel\n",
      "FANABNANATA000001_TWConv \t\t Conv2d\n",
      "FANABNANATA000001_LUTAct \t\t LUTActivation\n",
      "HPTin000009 \t\t HelperInputPrecisionTunnel\n",
      "HPTout000010 \t\t HelperOutputPrecisionTunnel\n",
      "HPTclone000010 \t\t ANAActivation\n",
      "O000010 \t\t MaxPool2d\n",
      "O000011 \t\t ANAConv2d\n",
      "O000012 \t\t BatchNorm2d\n",
      "O000013 \t\t ANAActivation\n",
      "HPTin000013 \t\t HelperInputPrecisionTunnel\n",
      "HPTout000014 \t\t HelperOutputPrecisionTunnel\n",
      "FANABNANATA000000_TWConv \t\t Conv2d\n",
      "FANABNANATA000000_LUTAct \t\t LUTActivation\n",
      "HPTin000016 \t\t HelperInputPrecisionTunnel\n",
      "HPTout000017 \t\t HelperOutputPrecisionTunnel\n",
      "HPTclone000017 \t\t ANAActivation\n",
      "O000017 \t\t MaxPool2d\n",
      "O000018 \t\t AdaptiveAvgPool2d\n",
      "O000019 \t\t ViewFlattenNd\n",
      "O000020 \t\t ANALinear\n",
      "O000021 \t\t BatchNorm1d\n",
      "O000022 \t\t ANAActivation\n",
      "HPTin000022 \t\t HelperInputPrecisionTunnel\n",
      "HPTout000023 \t\t HelperOutputPrecisionTunnel\n",
      "HPTclone000023 \t\t ANAActivation\n",
      "O000023 \t\t ANALinear\n",
      "O000024 \t\t BatchNorm1d\n",
      "O000025 \t\t ANAActivation\n",
      "HPTin000025 \t\t HelperInputPrecisionTunnel\n",
      "HPTout000026 \t\t HelperOutputPrecisionTunnel\n",
      "HPTclone000026 \t\t ANAActivation\n",
      "O000026 \t\t Linear\n",
      "HO000000 \t\t HelperOutput\n"
     ]
    }
   ],
   "source": [
    "for n in topological_sort(pytorche.G):\n",
    "    print(\"{} \\t\\t {}\".format(n, pytorche.nodes_dict[n].ntype))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f9f35202",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/spmatteo/anaconda3/envs/quantlab/lib/python3.8/site-packages/quantlib-0.1-py3.8.egg/quantlib/backends/cutie/grrules/ana/folding.py:54: RuntimeWarning: invalid value encountered in reciprocal\n",
      "  xi   = gamma * (sigma ** -1)\n"
     ]
    }
   ],
   "source": [
    "pytorche.resume()\n",
    "pytorche.set_grr(qb.cutie.grrules.ana.FoldANAActANAConvBNANAActTypeBRule())\n",
    "pytorche.edit()\n",
    "pytorche.pause()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f24d4f74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HI000000 \t\t HelperInput\n",
      "O000000 \t\t ANAConv2d\n",
      "O000001 \t\t BatchNorm2d\n",
      "O000002 \t\t ANAActivation\n",
      "HPTin000002 \t\t HelperInputPrecisionTunnel\n",
      "HPTout000003 \t\t HelperOutputPrecisionTunnel\n",
      "FANABNANATB000000_MaxPool \t\t MaxPool2d\n",
      "FANABNANATB000000_TWConv \t\t Conv2d\n",
      "FANABNANATB000000_LUTAct \t\t LUTActivation\n",
      "HPTin000006 \t\t HelperInputPrecisionTunnel\n",
      "HPTout000007 \t\t HelperOutputPrecisionTunnel\n",
      "FANABNANATA000001_TWConv \t\t Conv2d\n",
      "FANABNANATA000001_LUTAct \t\t LUTActivation\n",
      "HPTin000009 \t\t HelperInputPrecisionTunnel\n",
      "HPTout000010 \t\t HelperOutputPrecisionTunnel\n",
      "FANABNANATB000001_MaxPool \t\t MaxPool2d\n",
      "FANABNANATB000001_TWConv \t\t Conv2d\n",
      "FANABNANATB000001_LUTAct \t\t LUTActivation\n",
      "HPTin000013 \t\t HelperInputPrecisionTunnel\n",
      "HPTout000014 \t\t HelperOutputPrecisionTunnel\n",
      "FANABNANATA000000_TWConv \t\t Conv2d\n",
      "FANABNANATA000000_LUTAct \t\t LUTActivation\n",
      "HPTin000016 \t\t HelperInputPrecisionTunnel\n",
      "HPTout000017 \t\t HelperOutputPrecisionTunnel\n",
      "HPTclone000017 \t\t ANAActivation\n",
      "O000017 \t\t MaxPool2d\n",
      "O000018 \t\t AdaptiveAvgPool2d\n",
      "O000019 \t\t ViewFlattenNd\n",
      "O000020 \t\t ANALinear\n",
      "O000021 \t\t BatchNorm1d\n",
      "O000022 \t\t ANAActivation\n",
      "HPTin000022 \t\t HelperInputPrecisionTunnel\n",
      "HPTout000023 \t\t HelperOutputPrecisionTunnel\n",
      "HPTclone000023 \t\t ANAActivation\n",
      "O000023 \t\t ANALinear\n",
      "O000024 \t\t BatchNorm1d\n",
      "O000025 \t\t ANAActivation\n",
      "HPTin000025 \t\t HelperInputPrecisionTunnel\n",
      "HPTout000026 \t\t HelperOutputPrecisionTunnel\n",
      "HPTclone000026 \t\t ANAActivation\n",
      "O000026 \t\t Linear\n",
      "HO000000 \t\t HelperOutput\n"
     ]
    }
   ],
   "source": [
    "for n in topological_sort(pytorche.G):\n",
    "    print(\"{} \\t\\t {}\".format(n, pytorche.nodes_dict[n].ntype))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1acb8174",
   "metadata": {},
   "outputs": [],
   "source": [
    "pytorche.resume()\n",
    "pytorche.set_grr(qb.cutie.grrules.ana.FoldANAActANALinearBNANAActTypeARule())\n",
    "pytorche.edit()\n",
    "pytorche.pause()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a19ef75a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HI000000 \t\t HelperInput\n",
      "O000000 \t\t ANAConv2d\n",
      "O000001 \t\t BatchNorm2d\n",
      "O000002 \t\t ANAActivation\n",
      "HPTin000002 \t\t HelperInputPrecisionTunnel\n",
      "HPTout000003 \t\t HelperOutputPrecisionTunnel\n",
      "FANABNANATB000000_MaxPool \t\t MaxPool2d\n",
      "FANABNANATB000000_TWConv \t\t Conv2d\n",
      "FANABNANATB000000_LUTAct \t\t LUTActivation\n",
      "HPTin000006 \t\t HelperInputPrecisionTunnel\n",
      "HPTout000007 \t\t HelperOutputPrecisionTunnel\n",
      "FANABNANATA000001_TWConv \t\t Conv2d\n",
      "FANABNANATA000001_LUTAct \t\t LUTActivation\n",
      "HPTin000009 \t\t HelperInputPrecisionTunnel\n",
      "HPTout000010 \t\t HelperOutputPrecisionTunnel\n",
      "FANABNANATB000001_MaxPool \t\t MaxPool2d\n",
      "FANABNANATB000001_TWConv \t\t Conv2d\n",
      "FANABNANATB000001_LUTAct \t\t LUTActivation\n",
      "HPTin000013 \t\t HelperInputPrecisionTunnel\n",
      "HPTout000014 \t\t HelperOutputPrecisionTunnel\n",
      "FANABNANATA000000_TWConv \t\t Conv2d\n",
      "FANABNANATA000000_LUTAct \t\t LUTActivation\n",
      "HPTin000016 \t\t HelperInputPrecisionTunnel\n",
      "HPTout000017 \t\t HelperOutputPrecisionTunnel\n",
      "HPTclone000017 \t\t ANAActivation\n",
      "O000017 \t\t MaxPool2d\n",
      "O000018 \t\t AdaptiveAvgPool2d\n",
      "O000019 \t\t ViewFlattenNd\n",
      "O000020 \t\t ANALinear\n",
      "O000021 \t\t BatchNorm1d\n",
      "O000022 \t\t ANAActivation\n",
      "HPTin000022 \t\t HelperInputPrecisionTunnel\n",
      "HPTout000023 \t\t HelperOutputPrecisionTunnel\n",
      "FANABNANALinTA000000_TWLinear \t\t Linear\n",
      "FANABNANALinTA000000_LUTAct \t\t LUTActivation\n",
      "HPTin000025 \t\t HelperInputPrecisionTunnel\n",
      "HPTout000026 \t\t HelperOutputPrecisionTunnel\n",
      "HPTclone000026 \t\t ANAActivation\n",
      "O000026 \t\t Linear\n",
      "HO000000 \t\t HelperOutput\n"
     ]
    }
   ],
   "source": [
    "for n in topological_sort(pytorche.G):\n",
    "    print(\"{} \\t\\t {}\".format(n, pytorche.nodes_dict[n].ntype))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ae4976b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "pytorche.resume()\n",
    "pytorche.set_grr(qb.cutie.grrules.ana.FoldANAActANALinearBNANAActTypeBRule())\n",
    "pytorche.edit()\n",
    "pytorche.pause()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "0bedbea1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HI000000 \t\t HelperInput\n",
      "O000000 \t\t ANAConv2d\n",
      "O000001 \t\t BatchNorm2d\n",
      "O000002 \t\t ANAActivation\n",
      "HPTin000002 \t\t HelperInputPrecisionTunnel\n",
      "HPTout000003 \t\t HelperOutputPrecisionTunnel\n",
      "FANABNANATB000000_MaxPool \t\t MaxPool2d\n",
      "FANABNANATB000000_TWConv \t\t Conv2d\n",
      "FANABNANATB000000_LUTAct \t\t LUTActivation\n",
      "HPTin000006 \t\t HelperInputPrecisionTunnel\n",
      "HPTout000007 \t\t HelperOutputPrecisionTunnel\n",
      "FANABNANATA000001_TWConv \t\t Conv2d\n",
      "FANABNANATA000001_LUTAct \t\t LUTActivation\n",
      "HPTin000009 \t\t HelperInputPrecisionTunnel\n",
      "HPTout000010 \t\t HelperOutputPrecisionTunnel\n",
      "FANABNANATB000001_MaxPool \t\t MaxPool2d\n",
      "FANABNANATB000001_TWConv \t\t Conv2d\n",
      "FANABNANATB000001_LUTAct \t\t LUTActivation\n",
      "HPTin000013 \t\t HelperInputPrecisionTunnel\n",
      "HPTout000014 \t\t HelperOutputPrecisionTunnel\n",
      "FANABNANATA000000_TWConv \t\t Conv2d\n",
      "FANABNANATA000000_LUTAct \t\t LUTActivation\n",
      "HPTin000016 \t\t HelperInputPrecisionTunnel\n",
      "HPTout000017 \t\t HelperOutputPrecisionTunnel\n",
      "FANABNANALinTB000000_MaxPool \t\t MaxPool2d\n",
      "FANABNANALinTB000000_TWLinear \t\t Linear\n",
      "FANABNANALinTB000000_LUTAct \t\t LUTActivation\n",
      "HPTin000022 \t\t HelperInputPrecisionTunnel\n",
      "HPTout000023 \t\t HelperOutputPrecisionTunnel\n",
      "FANABNANALinTA000000_TWLinear \t\t Linear\n",
      "FANABNANALinTA000000_LUTAct \t\t LUTActivation\n",
      "HPTin000025 \t\t HelperInputPrecisionTunnel\n",
      "HPTout000026 \t\t HelperOutputPrecisionTunnel\n",
      "HPTclone000026 \t\t ANAActivation\n",
      "O000026 \t\t Linear\n",
      "HO000000 \t\t HelperOutput\n"
     ]
    }
   ],
   "source": [
    "for n in topological_sort(pytorche.G):\n",
    "    print(\"{} \\t\\t {}\".format(n, pytorche.nodes_dict[n].ntype))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3444e46",
   "metadata": {},
   "source": [
    "### **Exercise**: implementing a DPO GRR\n",
    "\n",
    "In the next cell, you will implement your first DPO GRR.\n",
    "In particular, you will complete the `core` method of the rule, by invoking the `fold_anaact_anaconv2d_bn2d_anaact` with the parameters that are correct for this rule, and creating the replacement PyTorch modules.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "da63e70d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FoldANAConvBNANAActRule(DPORule):\n",
    "\n",
    "    def __init__(self, lut_entry_bits=16):\n",
    "\n",
    "        self._lut_entry_bits = lut_entry_bits\n",
    "\n",
    "        # Nodes of the interface\n",
    "        K_types = OrderedDict()\n",
    "        K_types.update({'HPin':  qg.graphs.HelperInput.__name__})\n",
    "        K_types.update({'HPTin': qg.graphs.HelperInputPrecisionTunnel.__name__})\n",
    "        K_types = OrderedDict([('/'.join(['K-term', k]), v) for k, v in K_types.items()])\n",
    "\n",
    "        # Nodes in the core template graph\n",
    "        LK_types = OrderedDict()\n",
    "        LK_types.update({'ANAConv':   qa.ana.ANAConv2d.__name__})\n",
    "        LK_types.update({'BatchNorm': nn.BatchNorm2d.__name__})\n",
    "        LK_types.update({'ANAActout': qa.ana.ANAActivation.__name__})\n",
    "        LK_types = OrderedDict([('/'.join(['L-term', k]), v) for k, v in LK_types.items()])\n",
    "\n",
    "        # Nodes in the core replacement graph\n",
    "        RK_types = OrderedDict()\n",
    "        RK_types.update({'TWConv': nn.Conv2d.__name__})\n",
    "        RK_types.update({'LUTAct': LUTActivation.__name__})\n",
    "        RK_types = OrderedDict([('/'.join(['R-term', k]), v) for k, v in RK_types.items()])\n",
    "\n",
    "        K_node_IDs  = list(K_types.keys())\n",
    "        LK_node_IDs = list(LK_types.keys())\n",
    "        RK_node_IDs = list(RK_types.keys())\n",
    "\n",
    "        # define the template graph L [L-term]\n",
    "        L_node_IDs = [K_node_IDs[0]] + LK_node_IDs + [K_node_IDs[-1]]\n",
    "        self.L = nx.DiGraph()\n",
    "        # Define arcs between nodes in full template graph\n",
    "        self.L.add_edges_from({(u, v) for u, v in zip(L_node_IDs[:-1], L_node_IDs[1:])})\n",
    "\n",
    "        # Here, graph is only operation nodes\n",
    "        # Necessary for seeker\n",
    "        nx.set_node_attributes(self.L, {vL: Bipartite.KERNEL for vL in set(self.L.nodes)}, 'bipartite')\n",
    "        nx.set_node_attributes(self.L, {**K_types, **LK_types}, 'type')\n",
    "\n",
    "        # define the context (sub-)graph K [K-term]\n",
    "        VK = set(K_node_IDs)  # precision tunnel nodes define the context graph\n",
    "        self.K = self.L.subgraph(VK)\n",
    "\n",
    "        # define the template (sub-)graph L\\K\n",
    "        VLK = set(self.L.nodes).difference(set(self.K.nodes))\n",
    "        self.LK = self.L.subgraph(VLK)\n",
    "\n",
    "        # define the replacement (sub-)graph R\\K [\"gluing\" R\\K to K yields the graph R, i.e., the R-term]\n",
    "        self.RK = nx.DiGraph()\n",
    "        self.RK.add_edges_from({(u, v) for u, v in zip(RK_node_IDs[:-1], RK_node_IDs[1:])})\n",
    "        nx.set_node_attributes(self.RK, {vRK: Bipartite.KERNEL for vRK in set(self.RK.nodes)}, 'bipartite')\n",
    "        nx.set_node_attributes(self.RK, RK_types, 'type')\n",
    "\n",
    "        # define the arcs that go from the vertices of K to those of R\\K, and viceversa\n",
    "        E_K2RK = {(K_node_IDs[0], RK_node_IDs[0])}\n",
    "        E_RK2K = {(RK_node_IDs[-1], K_node_IDs[-1])}\n",
    "        E_K2RK2K = E_K2RK | E_RK2K\n",
    "        # disintegrate `E_K2RK` and `E_RK2K` along fibres to speed up rule application\n",
    "        # A fibre is kind of like fixing one argument of a two input one output function and looking at all possible outputs\n",
    "        self.F_K2RK = {vK: set(arc for arc in E_K2RK if arc[0] == vK) for vK in set(self.K.nodes)}\n",
    "        self.F_RK2K = {vK: set(arc for arc in E_RK2K if arc[1] == vK) for vK in set(self.K.nodes)}\n",
    "\n",
    "        # # glue together the (sub-)graphs L\\K and R\\K along the vertices of K\n",
    "        # self.S = nx.compose(self.L, self.RK)\n",
    "        # self.S.add_edges_from(E_K2RK2K)\n",
    "\n",
    "        # since the GRR's L-term has been modified, rebuild the seeker\n",
    "        self.seeker = Seeker(self.L)\n",
    "\n",
    "        # this machinery can generate always-new identifiers for different rule applications\n",
    "        self._counter = itertools.count()\n",
    "\n",
    "    def _get_rule_count(self):\n",
    "        rule_count = ''.join(['FConvBNANA', __NODE_ID_FORMAT__.format(next(self._counter))])\n",
    "        return rule_count\n",
    "\n",
    "    def core(self, HI, g, nodes_dict):\n",
    "\n",
    "        # generate the substitute (sub-)graph J\\I\n",
    "        rule_count = self._get_rule_count()\n",
    "        g_RK2JI = {vRK: '_'.join([rule_count, vRK.replace('R-term/', '')]) for vRK in set(self.RK.nodes)}\n",
    "        JI = nx.relabel_nodes(self.RK, g_RK2JI, copy=True)\n",
    "\n",
    "        # get pointers to the old modules;\n",
    "        # these pointers will enable two actions:\n",
    "        #   1. extracting the arguments required to perform the folding\n",
    "        #   2. extracting the parameters to instantiate the new modules\n",
    "        g_L2H = {vL: vH for vH, vL in g.items()}\n",
    "        mconv2d = nodes_dict[g_L2H['/'.join(['L-term', 'ANAConv'])]].nobj\n",
    "        mbn2d   = nodes_dict[g_L2H['/'.join(['L-term', 'BatchNorm'])]].nobj\n",
    "        manaout = nodes_dict[g_L2H['/'.join(['L-term', 'ANAActout'])]].nobj\n",
    "\n",
    "        # fold\n",
    "        tau, weight = fold_anaact_anaconv2d_bn2d_anaact(torch.Tensor([1.0]),\n",
    "                                                        mconv2d.eps, mconv2d.weight_maybe_quant,\n",
    "                                                        mbn2d.running_mean, mbn2d.running_var, mbn2d.eps, mbn2d.weight, mbn2d.bias,\n",
    "                                                        manaout.eps,\n",
    "                                                        manaout.thresholds,\n",
    "                                                        ceiltau=False)\n",
    "        \n",
    "        # build the new modules\n",
    "        mtwconv = nn.Conv2d(mconv2d.in_channels, mconv2d.out_channels, mconv2d.kernel_size,\n",
    "                            stride=mconv2d.stride, padding=mconv2d.padding, dilation=mconv2d.dilation, groups=mconv2d.groups,\n",
    "                            bias=mconv2d.bias is not None).to(torch.device('cpu'))\n",
    "        mtwconv.weight.data = weight\n",
    "\n",
    "        mlutact = LUTActivation(tau, manaout.quant_levels)\n",
    "\n",
    "        # register the newly created nodes\n",
    "        vJI_2_ptnode = {}\n",
    "        vJI_2_ptnode[g_RK2JI['/'.join(['R-term', 'TWConv'])]] = PyTorchNode(mtwconv)\n",
    "        vJI_2_ptnode[g_RK2JI['/'.join(['R-term', 'LUTAct'])]] = PyTorchNode(mlutact)\n",
    "\n",
    "        return JI, vJI_2_ptnode\n",
    "\n",
    "    # G: Full/original graph\n",
    "    # nodes_dict: Mapping between node identifiers of G and actual underlying objects\n",
    "    # g: One instance of all occurences of the template in G, i.e. one application point for the replacement rule -> one morphism\n",
    "    def apply(self, G, nodes_dict, g):\n",
    "\n",
    "        # create new containers\n",
    "        G = G.copy()\n",
    "        # Dictionary mapping of node identifiers to a payload\n",
    "        # keys in nodes_dict should be the same as G.nodes\n",
    "        nodes_dict = {**nodes_dict}\n",
    "\n",
    "        # characterise the match graph H\n",
    "        # Occurence of template in the graph\n",
    "        # SPMATTEO: Some assumptions to discuss\n",
    "        VI = {vH for vH, vL in g.items() if vL in set(self.K.nodes)} # Occurence of context\n",
    "        VHI = {vH for vH, vL in g.items() if vL not in set(self.K.nodes)} # Occurence of core template\n",
    "        HI = G.subgraph(VHI) # HI is the subgraph induced by the set of nodes VHI\n",
    "        \n",
    "        # generate the substitute (sub-)graph J\\I (completely detached from G)\n",
    "        # Instantiate blueprint of the replacement graph\n",
    "        JI, vJI_2_ptnode = self.core(HI, g, nodes_dict)\n",
    "\n",
    "        # add the substitute (sub-)graph J\\I to the main graph G\n",
    "        G = nx.compose(G, JI) # G now has two connected but 'independent' subgraphs\n",
    "        nodes_dict.update(vJI_2_ptnode) # Add new payloads from substitute graph\n",
    "\n",
    "        # glue the substitute (sub-)graph J\\I to the interface (sub-)graph I\n",
    "        JI2RK_morphisms = Seeker(self.RK).get_morphisms(JI)\n",
    "        assert len(JI2RK_morphisms) == 1\n",
    "        g_JI2RK = JI2RK_morphisms[0]\n",
    "        g_RK2JI = {vRK: vJI for vJI, vRK in g_JI2RK.items()}\n",
    "\n",
    "        for vI in VI: # for each node in the interface subgraph of G\n",
    "            vK = g[vI]\n",
    "            G.add_edges_from({(vI, g_RK2JI[vRK]) for (_, vRK) in self.F_K2RK[vK]}) # incoming interface connections from G to substitute graph\n",
    "            G.add_edges_from({(g_RK2JI[vRK], vI) for (vRK, _) in self.F_RK2K[vK]}) # outcoming interface connections from substitute graph to G\n",
    "            # the new modules are fully integerized, so the precision tunnel should not embed integer numbers in floating point numbers\n",
    "            # Specific to integer arithmetic transformation -> No relation to graph editing, per-se\n",
    "            if nodes_dict[vI].ntype == qg.graphs.HelperInput.__name__:\n",
    "                pass\n",
    "            elif nodes_dict[vI].ntype == qg.graphs.HelperInputPrecisionTunnel.__name__:\n",
    "                nodes_dict[vI] = PyTorchNode(qg.graphs.HelperInputPrecisionTunnel(1.0))\n",
    "            else:\n",
    "                raise TypeError  # interface nodes should be objects of class `qg.graphs.HelperPrecisionTunnel` only\n",
    "\n",
    "        # discard the match (sub-)graph H\\I\n",
    "        # Assumption: removing a node also removes all arcs pointing to or from that node\n",
    "        G.remove_nodes_from(set(HI.nodes))\n",
    "\n",
    "        # Remove the payload, i.e. underying objects, accordingly\n",
    "        for vHI in VHI:\n",
    "            del nodes_dict[vHI]\n",
    "\n",
    "        return G, nodes_dict\n",
    "\n",
    "    def seek(self, G, nodes_dict):\n",
    "        gs = self.seeker.get_morphisms(G)\n",
    "        return gs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "513e5d93",
   "metadata": {},
   "outputs": [],
   "source": [
    "pytorche.resume()\n",
    "pytorche.set_grr(qb.cutie.grrules.ana.FoldANAConvBNANAActRule())\n",
    "pytorche.edit()\n",
    "pytorche.pause()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a841623e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HI000000 \t\t HelperInput\n",
      "FConvBNANA000000_TWConv \t\t Conv2d\n",
      "FConvBNANA000000_LUTAct \t\t LUTActivation\n",
      "HPTin000002 \t\t HelperInputPrecisionTunnel\n",
      "HPTout000003 \t\t HelperOutputPrecisionTunnel\n",
      "FANABNANATB000000_MaxPool \t\t MaxPool2d\n",
      "FANABNANATB000000_TWConv \t\t Conv2d\n",
      "FANABNANATB000000_LUTAct \t\t LUTActivation\n",
      "HPTin000006 \t\t HelperInputPrecisionTunnel\n",
      "HPTout000007 \t\t HelperOutputPrecisionTunnel\n",
      "FANABNANATA000001_TWConv \t\t Conv2d\n",
      "FANABNANATA000001_LUTAct \t\t LUTActivation\n",
      "HPTin000009 \t\t HelperInputPrecisionTunnel\n",
      "HPTout000010 \t\t HelperOutputPrecisionTunnel\n",
      "FANABNANATB000001_MaxPool \t\t MaxPool2d\n",
      "FANABNANATB000001_TWConv \t\t Conv2d\n",
      "FANABNANATB000001_LUTAct \t\t LUTActivation\n",
      "HPTin000013 \t\t HelperInputPrecisionTunnel\n",
      "HPTout000014 \t\t HelperOutputPrecisionTunnel\n",
      "FANABNANATA000000_TWConv \t\t Conv2d\n",
      "FANABNANATA000000_LUTAct \t\t LUTActivation\n",
      "HPTin000016 \t\t HelperInputPrecisionTunnel\n",
      "HPTout000017 \t\t HelperOutputPrecisionTunnel\n",
      "FANABNANALinTB000000_MaxPool \t\t MaxPool2d\n",
      "FANABNANALinTB000000_TWLinear \t\t Linear\n",
      "FANABNANALinTB000000_LUTAct \t\t LUTActivation\n",
      "HPTin000022 \t\t HelperInputPrecisionTunnel\n",
      "HPTout000023 \t\t HelperOutputPrecisionTunnel\n",
      "FANABNANALinTA000000_TWLinear \t\t Linear\n",
      "FANABNANALinTA000000_LUTAct \t\t LUTActivation\n",
      "HPTin000025 \t\t HelperInputPrecisionTunnel\n",
      "HPTout000026 \t\t HelperOutputPrecisionTunnel\n",
      "HPTclone000026 \t\t ANAActivation\n",
      "O000026 \t\t Linear\n",
      "HO000000 \t\t HelperOutput\n"
     ]
    }
   ],
   "source": [
    "for n in topological_sort(pytorche.G):\n",
    "    print(\"{} \\t\\t {}\".format(n, pytorche.nodes_dict[n].ntype))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "8389bf27",
   "metadata": {},
   "outputs": [],
   "source": [
    "pytorche.resume()\n",
    "pytorche.set_grr(qb.cutie.grrules.ana.FoldANAActLinearRule())\n",
    "pytorche.edit()\n",
    "pytorche.pause()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b804c53",
   "metadata": {},
   "source": [
    "After applying all the GRRs, we should see that the derived `PyTorchGraph` consist only of helper and supported modules.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "78d4ee12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HI000000 \t\t HelperInput\n",
      "FConvBNANA000000_TWConv \t\t Conv2d\n",
      "FConvBNANA000000_LUTAct \t\t LUTActivation\n",
      "HPTin000002 \t\t HelperInputPrecisionTunnel\n",
      "HPTout000003 \t\t HelperOutputPrecisionTunnel\n",
      "FANABNANATB000000_MaxPool \t\t MaxPool2d\n",
      "FANABNANATB000000_TWConv \t\t Conv2d\n",
      "FANABNANATB000000_LUTAct \t\t LUTActivation\n",
      "HPTin000006 \t\t HelperInputPrecisionTunnel\n",
      "HPTout000007 \t\t HelperOutputPrecisionTunnel\n",
      "FANABNANATA000001_TWConv \t\t Conv2d\n",
      "FANABNANATA000001_LUTAct \t\t LUTActivation\n",
      "HPTin000009 \t\t HelperInputPrecisionTunnel\n",
      "HPTout000010 \t\t HelperOutputPrecisionTunnel\n",
      "FANABNANATB000001_MaxPool \t\t MaxPool2d\n",
      "FANABNANATB000001_TWConv \t\t Conv2d\n",
      "FANABNANATB000001_LUTAct \t\t LUTActivation\n",
      "HPTin000013 \t\t HelperInputPrecisionTunnel\n",
      "HPTout000014 \t\t HelperOutputPrecisionTunnel\n",
      "FANABNANATA000000_TWConv \t\t Conv2d\n",
      "FANABNANATA000000_LUTAct \t\t LUTActivation\n",
      "HPTin000016 \t\t HelperInputPrecisionTunnel\n",
      "HPTout000017 \t\t HelperOutputPrecisionTunnel\n",
      "FANABNANALinTB000000_MaxPool \t\t MaxPool2d\n",
      "FANABNANALinTB000000_TWLinear \t\t Linear\n",
      "FANABNANALinTB000000_LUTAct \t\t LUTActivation\n",
      "HPTin000022 \t\t HelperInputPrecisionTunnel\n",
      "HPTout000023 \t\t HelperOutputPrecisionTunnel\n",
      "FANABNANALinTA000000_TWLinear \t\t Linear\n",
      "FANABNANALinTA000000_LUTAct \t\t LUTActivation\n",
      "HPTin000025 \t\t HelperInputPrecisionTunnel\n",
      "HPTout000026 \t\t HelperOutputPrecisionTunnel\n",
      "FANAActLinear000000_Linear \t\t Linear\n",
      "HO000000 \t\t HelperOutput\n"
     ]
    }
   ],
   "source": [
    "for n in topological_sort(pytorche.G):\n",
    "    print(\"{} \\t\\t {}\".format(n, pytorche.nodes_dict[n].ntype))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eeec8b1",
   "metadata": {},
   "source": [
    "### Removing *helper* nodes\n",
    "\n",
    "Before generating code for the true-quantized network, we need to remove the helper nodes.\n",
    "This can be done using rules defined in the `quantlib/editing/graphs/grrules/helper.py` module.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "a154bb49",
   "metadata": {},
   "outputs": [],
   "source": [
    "pytorche.resume()\n",
    "pytorche.set_grr(qg.grrules.RemovePrecisionTunnelRule())\n",
    "pytorche.edit()\n",
    "pytorche.set_grr(qg.grrules.RemoveInputNodeRule())\n",
    "pytorche.edit()\n",
    "pytorche.set_grr(qg.grrules.RemoveOutputNodeRule())\n",
    "pytorche.edit()\n",
    "pytorche.pause()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "9f43b473",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FConvBNANA000000_TWConv \t\t Conv2d\n",
      "FConvBNANA000000_LUTAct \t\t LUTActivation\n",
      "FANABNANATB000000_MaxPool \t\t MaxPool2d\n",
      "FANABNANATB000000_TWConv \t\t Conv2d\n",
      "FANABNANATB000000_LUTAct \t\t LUTActivation\n",
      "FANABNANATA000001_TWConv \t\t Conv2d\n",
      "FANABNANATA000001_LUTAct \t\t LUTActivation\n",
      "FANABNANATB000001_MaxPool \t\t MaxPool2d\n",
      "FANABNANATB000001_TWConv \t\t Conv2d\n",
      "FANABNANATB000001_LUTAct \t\t LUTActivation\n",
      "FANABNANATA000000_TWConv \t\t Conv2d\n",
      "FANABNANATA000000_LUTAct \t\t LUTActivation\n",
      "FANABNANALinTB000000_MaxPool \t\t MaxPool2d\n",
      "FANABNANALinTB000000_TWLinear \t\t Linear\n",
      "FANABNANALinTB000000_LUTAct \t\t LUTActivation\n",
      "FANABNANALinTA000000_TWLinear \t\t Linear\n",
      "FANABNANALinTA000000_LUTAct \t\t LUTActivation\n",
      "FANAActLinear000000_Linear \t\t Linear\n"
     ]
    }
   ],
   "source": [
    "for n in topological_sort(pytorche.G):\n",
    "    print(\"{} \\t\\t {}\".format(n, pytorche.nodes_dict[n].ntype))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4d96b27",
   "metadata": {},
   "source": [
    "### Validating the conversion\n",
    "\n",
    "At the moment, we are still working on code generation functionalities.\n",
    "Therefore, we will conclude this tutorial by manually deriving a true-quantized PyTorch network object, writing a mock-up `forward` method for it, and comparing the performance of the fake-quantized network with that of the true-quantized one.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "58fdb274",
   "metadata": {},
   "outputs": [],
   "source": [
    "truequantnet = nn.Sequential(*[pytorche.nodes_dict[n].nobj for n in topological_sort(pytorche.G)])\n",
    "\n",
    "def truequantnet_forward(truequantnet: nn.Module,\n",
    "                         x:            torch.Tensor):\n",
    "    y = x\n",
    "    for i in range(0, 13):\n",
    "        y = truequantnet[i](y)\n",
    "    y = y.view(y.shape[0], -1)\n",
    "    for i in range(13, 18):\n",
    "        y = truequantnet[i](y)\n",
    "    \n",
    "    return y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "e6daa5f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Fake-quantized network accuracy:             10.800%\n",
      "True-quantized network accuracy:             10.800%\n",
      "Agreement percentage between fake and true: 100.000%\n"
     ]
    }
   ],
   "source": [
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "\n",
    "transform   = transforms.Compose([transforms.ToTensor(),\n",
    "                                  transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2470, 0.2430, 0.2610))])\n",
    "test_set    = torchvision.datasets.CIFAR10(root=os.path.join(os.curdir, 'data'), train=False, download=True, transform=transform)\n",
    "test_loader = torch.utils.data.DataLoader(test_set, batch_size=1, shuffle=True, num_workers=1)\n",
    "\n",
    "fakenet = net\n",
    "truenet = truequantnet\n",
    "\n",
    "to_test      = 500  # checking all 10k test images can take too much time on CPU...\n",
    "assert 0 < to_test <= len(test_set)\n",
    "\n",
    "fake_correct = 0\n",
    "true_correct = 0\n",
    "agree        = 0\n",
    "\n",
    "for i, (x, y) in enumerate(test_loader):\n",
    "    \n",
    "    if i < to_test:\n",
    "        yfake = fakenet(x)\n",
    "        ytrue = truequantnet_forward(truenet, x)\n",
    "\n",
    "        fake_correct += torch.argmax(yfake) == y\n",
    "        true_correct += torch.argmax(ytrue) == y\n",
    "        agree        += torch.argmax(ytrue) == torch.argmax(yfake)\n",
    "\n",
    "print(\"Fake-quantized network accuracy:            {:7.3f}%\".format(100 * fake_correct.item() / to_test))\n",
    "print(\"True-quantized network accuracy:            {:7.3f}%\".format(100 * true_correct.item() / to_test))\n",
    "print(\"Agreement percentage between fake and true: {:7.3f}%\".format(100 * agree.item()        / to_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f49c740",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

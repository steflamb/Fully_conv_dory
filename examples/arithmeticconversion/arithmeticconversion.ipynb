{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a9711879",
   "metadata": {},
   "source": [
    "# Arithmetic conversion: *folding* and *casting*\n",
    "\n",
    "Apart from graph rewriting, *fake2true* conversions require reordering arithmetic operations, folding together some operands, and casting the results to integer or other low-precision data formats, while at the same time preserving functional equivalence (or at least minimising the discrepancy between the original sequence of operations and the resulting one).\n",
    "We refer to this part of the conversion as *arithmetic conversion*.\n",
    "\n",
    "In the case of machine learning on microcontrollers, the final sequence of operations must be composed by operations and involve operands that can be interpreted by the *instruction set architecture* (ISA) of the target hardware device that will execute the program.\n",
    "\n",
    "\n",
    "### *Folding*\n",
    "\n",
    "We call *folding* each sequence of applications of basic arithmetical properties or identity decompositions used to transform a given sequence of operations into an equivalent one.\n",
    "Commonly-used arithmetical properties are:\n",
    "\n",
    "* the *commutative* property: $a + b = b + a$;\n",
    "* the *associative* property: $(a + b) + c = a + (b + c)$;\n",
    "* the *distributive* property and its inverse: $(a + b) * c = (a * c) + (b * c)$ and $(a * c) + (b * c) = (a + b) * c$.\n",
    "\n",
    "Given a non-zero number $z \\in \\mathbb{R}$, the identity decompositions are the following:\n",
    "\n",
    "* *sum-and-subtract*: $x = x - z + z$;\n",
    "* *divide-and-multiply*: $ x = (x / z) * z$.\n",
    "\n",
    "For example, consider the operation of normalising a scalar product.\n",
    "Let $n > 0$ be an integer, $\\mathbb{R}^{n}$ be the $n$-dimensional Euclidean space, and $\\mathbf{x}, \\mathbf{w} \\in \\mathbb{R}^{n}$ be vectors.\n",
    "Let also $\\mu \\in \\mathbb{R},\\, \\sigma \\in \\mathbb{R}^{+}$, and consider the following operation:\n",
    "\n",
    "\\begin{equation*}\n",
    "    \\frac{\\langle \\mathbf{x}, \\mathbf{w} \\rangle - \\mu}{\\sigma} \\,.\n",
    "\\end{equation*}\n",
    "\n",
    "Folding rules (in this specific case, the distributive property) allow us to rewrite this operation as\n",
    "\n",
    "\\begin{equation*}\n",
    "\\begin{split}\n",
    "    \\frac{\\langle \\mathbf{x}, \\mathbf{w} \\rangle - \\mu}{\\sigma}\n",
    "    &= \\left( \\langle \\mathbf{x}, \\mathbf{w} \\rangle - \\mu \\right) \\frac{1}{\\sigma} \\\\\n",
    "    &= \\langle \\mathbf{x}, \\mathbf{w} \\rangle \\frac{1}{\\sigma} - \\frac{\\mu}{\\sigma} \\\\\n",
    "    &= \\langle \\mathbf{x}, \\mathbf{w} \\rangle \\tilde{\\sigma} + \\tilde{\\mu} \\,.\n",
    "\\end{split}\n",
    "\\end{equation*}\n",
    "\n",
    "The critical point here is that the operands $\\tilde{\\sigma} = 1/\\sigma \\in \\mathbb{R}^{+}$ and $\\tilde{\\mu} = -\\mu/\\sigma$ can be **pre-computed** starting from the operands $\\mu$ and $\\sigma$.\n",
    "The original version of the operation involves computing the scalar product $\\langle \\mathbf{x}, \\mathbf{w} \\rangle = \\sum_{i=1}^{n} x_{i} w_{i}$ ($n$ multiplications and $n-1$ sums), one subtraction, and one division.\n",
    "By contrast, the folded version still requires computing the scalar product, but then uses a multiplication and an addition.\n",
    "\n",
    "In this case, we are still performing the same number of operations, although with different operands.\n",
    "In other cases, though, the benefits of folding can become more apparent.\n",
    "For example, instead of supposing $\\mathbf{x}, \\mathbf{w} \\in \\mathbb{R}^{n}$, we can imagine that the components of these vectors take values in specific spaces of the form\n",
    "\n",
    "\\begin{equation*}\n",
    "    \\mathbb{Z}_{\\epsilon} := \\{ \\epsilon i \\,|\\, i \\in \\mathbb{Z} \\} \\,,\n",
    "\\end{equation*}\n",
    "\n",
    "where $\\epsilon \\in \\mathbb{R}^{+}$ is a positive constant.\n",
    "In particular, given constants $\\epsilon_{\\mathbf{x}}, \\epsilon_{\\mathbf{w}} > 0$ we assume that $\\mathbf{x} \\in \\mathbb{Z}_{\\epsilon_{\\mathbf{x}}}^{n}$ and $\\mathbf{w} \\in \\mathbb{Z}_{\\epsilon_{\\mathbf{w}}}^{n}$.\n",
    "Due to the definition of the spaces $\\mathbb{Z}_{\\epsilon}$, we can make explicit the fact that each $\\mathbf{x} \\in \\mathbb{Z}_{\\epsilon_{\\mathbf{x}}}^{n}$ can be rewritten as\n",
    "\n",
    "\\begin{equation*}\n",
    "    \\mathbf{x} = \\epsilon_{\\mathbf{x}} \\hat{\\mathbf{x}} \\,,\n",
    "\\end{equation*}\n",
    "\n",
    "where $\\hat{\\mathbf{x}} \\in \\mathbb{Z}^{n}$ if a vector with integer components $\\hat{x}_{i} \\in \\mathbb{Z}, \\, i = 1, \\dots, n$.\n",
    "An analogous representation is available for $\\mathbf{w} \\in \\mathbb{Z}_{\\epsilon_{\\mathbf{w}}}^{n}$: $\\mathbf{w} = \\epsilon_{\\mathbf{w}} \\hat{\\mathbf{w}}$.\n",
    "\n",
    "Given parameters $\\gamma, \\beta \\in \\mathbb{R}$ and a positive parameter $\\epsilon_{\\mathbf{s}} > 0$, a conversion that is often required in fake-to-true network transformations is the following:\n",
    "\n",
    "\\begin{equation*}\n",
    "\\begin{split}\n",
    "    \\frac{\\left( \\frac{\\langle \\mathbf{x}, \\mathbf{w} \\rangle - \\mu}{\\sigma} \\right) \\gamma + \\beta}{\\epsilon_{\\mathbf{s}}}\n",
    "    &= \\frac{\\left( \\frac{\\langle \\epsilon_{\\mathbf{x}} \\hat{\\mathbf{x}}, \\epsilon_{\\mathbf{w}} \\hat{\\mathbf{w}} \\rangle - \\mu}{\\sigma} \\right) \\gamma + \\beta}{\\epsilon_{\\mathbf{s}}} \\\\\n",
    "    &= \\left( \\left( \\frac{ \\sum_{i=1}^{n} \\epsilon_{\\mathbf{x}} \\hat{x}_{i} \\epsilon_{\\mathbf{w}} \\hat{w}_{i} - \\mu}{\\sigma} \\right) \\gamma + \\beta \\right) \\frac{1}{\\epsilon_{\\mathbf{s}}} \\\\\n",
    "    &= \\left( \\left( \\frac{ \\epsilon_{\\mathbf{x}} \\epsilon_{\\mathbf{w}} \\sum_{i=1}^{n} \\hat{x}_{i} \\hat{w}_{i} - \\mu}{\\sigma} \\right) \\gamma + \\beta \\right) \\frac{1}{\\epsilon_{\\mathbf{s}}} \\\\\n",
    "    &= \\left( \\left( \\frac{ \\epsilon_{\\mathbf{x}} \\epsilon_{\\mathbf{w}} \\langle \\hat{\\mathbf{x}}, \\hat{\\mathbf{w}} \\rangle - \\mu}{\\sigma} \\right) \\gamma + \\beta \\right) \\frac{1}{\\epsilon_{\\mathbf{s}}} \\\\\n",
    "    &= \\left( \\langle \\hat{\\mathbf{x}}, \\hat{\\mathbf{w}} \\rangle \\left( \\frac{\\epsilon_{\\mathbf{x}} \\epsilon_{\\mathbf{w}} \\gamma}{\\sigma} \\right) + \\left( \\frac{\\sigma \\beta - \\mu \\gamma}{\\sigma} \\right) \\right) \\frac{1}{\\epsilon_{\\mathbf{s}}} \\,.\n",
    "\\end{split}\n",
    "\\end{equation*}\n",
    "\n",
    "At this point, the folding process can evolve in two directions: the *add-then-multiply*, and the *multiply-then-add*.\n",
    "The *add-then-multiply* folding strategy divides the first factor by $(\\epsilon_{\\mathbf{x}} \\epsilon_{\\mathbf{w}} \\gamma)/\\sigma$ and multiplies the second by the same value, obtaining\n",
    "\n",
    "\\begin{equation*}\n",
    "\\begin{split}\n",
    "    \\frac{\\left( \\frac{\\langle \\mathbf{x}, \\mathbf{w} \\rangle - \\mu}{\\sigma} \\right) \\gamma + \\beta}{\\epsilon_{\\mathbf{s}}}\n",
    "    &= \\left( \\langle \\hat{\\mathbf{x}}, \\hat{\\mathbf{w}} \\rangle + \\left( \\frac{\\sigma \\beta - \\mu \\gamma}{\\epsilon_{\\mathbf{x}} \\epsilon_{\\mathbf{w}} \\gamma} \\right) \\right) \\frac{\\epsilon_{\\mathbf{x}} \\epsilon_{\\mathbf{w}} \\sigma}{\\epsilon_{\\mathbf{s}} \\sigma} \\\\\n",
    "    &= \\left( \\langle \\hat{\\mathbf{x}}, \\hat{\\mathbf{w}} \\rangle + \\beta' \\right) \\gamma' \\,,\n",
    "\\end{split}\n",
    "\\end{equation*}\n",
    "\n",
    "where $\\beta' = (\\sigma \\beta - \\mu \\gamma)/(\\epsilon_{\\mathbf{x}} \\epsilon_{\\mathbf{w}} \\gamma)$ and $\\sigma' = (\\epsilon_{\\mathbf{x}} \\epsilon_{\\mathbf{w}} \\gamma)/(\\epsilon_{\\mathbf{s}} \\sigma)$.\n",
    "The *multiply-then-add* folding strategy directly multiplies the first factor by $1/\\epsilon_{\\mathbf{s}}$, yielding\n",
    "\n",
    "\\begin{equation*}\n",
    "\\begin{split}\n",
    "    \\frac{\\left( \\frac{\\langle \\mathbf{x}, \\mathbf{w} \\rangle - \\mu}{\\sigma} \\right) \\gamma + \\beta}{\\epsilon_{\\mathbf{s}}}\n",
    "    &= \\langle \\hat{\\mathbf{x}}, \\hat{\\mathbf{w}} \\rangle \\left( \\frac{\\epsilon_{\\mathbf{x}} \\epsilon_{\\mathbf{w}} \\gamma}{\\epsilon_{\\mathbf{s}} \\sigma} \\right) + \\left( \\frac{\\sigma \\beta - \\mu \\gamma}{\\epsilon_{\\mathbf{x}} \\sigma} \\right) \\\\\n",
    "    &= \\langle \\hat{\\mathbf{x}}, \\hat{\\mathbf{w}} \\rangle \\gamma'' + \\beta'' \\,,\n",
    "\\end{split}\n",
    "\\end{equation*}\n",
    "\n",
    "where $\\beta'' = (\\sigma \\beta - \\mu \\gamma)/(\\epsilon_{\\mathbf{s}} \\sigma)$ and $\\sigma'' = \\sigma' = (\\epsilon_{\\mathbf{x}} \\epsilon_{\\mathbf{w}} \\gamma)/(\\epsilon_{\\mathbf{s}} \\sigma)$.\n",
    "\n",
    "Before folding, the original expression involves $n$ products and $n-1$ sums between *decimal* numbers (assuming that $\\epsilon_{\\mathbf{x}}$ and $\\epsilon_{\\mathbf{w}}$ are decimal numbers), a subtraction, a division, a multiplication, an addition, and another division.\n",
    "After folding, the expression involves $n$ products and $n-1$ sums between *integer* numbers, a multiplication, and an addition.\n",
    "By folding, we have derived a more compact version of the original expression, saving three operations.\n",
    "Nevertheless, the main benefit from the perspective of digital arithmetic is that performing multiplications and additions between integers can be much more efficient than performing it between decimal numbers (fixed-point and especially floating-point).\n",
    "\n",
    "**Warning**: the assumption of folding is that transformations based on the commutative, associative, and distributive properties as well as on identity decompositions yield *exactly* equivalent operations.\n",
    "In digital arithmetic, this is usually not the case.\n",
    "For example, consider the possible ways to compute $\\sigma'' = \\sigma' = (\\epsilon_{\\mathbf{x}} \\epsilon_{\\mathbf{w}} \\gamma)/(\\epsilon_{\\mathbf{s}} \\sigma) = \\epsilon_{\\mathbf{x}} \\epsilon_{\\mathbf{w}} \\gamma (1/\\epsilon_{\\mathbf{s}}) (1/\\sigma)$.\n",
    "Assuming that all the numbers in the expression are represented as floating-points, it might for example happen that\n",
    "\n",
    "\\begin{equation*}\n",
    "    \\left( \\left( \\epsilon_{\\mathbf{x}} \\epsilon_{\\mathbf{w}} \\right) \\gamma \\right) \\left( \\frac{1}{\\epsilon_{\\mathbf{s}}} \\frac{1}{\\sigma} \\right)\n",
    "    \\neq\n",
    "    \\left( \\left( \\left( \\epsilon_{\\mathbf{x}} \\epsilon_{\\mathbf{w}} \\right) \\gamma \\right) \\frac{1}{\\epsilon_{\\mathbf{s}}} \\right) \\left( \\frac{1}{\\sigma} \\right) \\,.\n",
    "\\end{equation*}\n",
    "\n",
    "\n",
    "### *Casting*\n",
    "\n",
    "Casting is the process by which digital data formats are converted into other data formats.\n",
    "An example of casting is transforming decimal numbers represented using a floating-point data format into decimal number represented using a fixed-point data format.\n",
    "It is usually the case that casting is a non-invertible transformation.\n",
    "\n",
    "\n",
    "### **Exercise**: folding a dot product involving fake-quantized vectors with a batch normalisation operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "07b2b22c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original value:                -21.857143\n",
      "Folded add-and-multiply value: -21.857143\n",
      "Folded multiply-and-add value: -21.857143\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "# operation parameters\n",
    "n     = 128\n",
    "\n",
    "eps_x = 2.0\n",
    "x_hat = torch.randint(low=-1, high=1+1, size=(n,))\n",
    "x     = eps_x * x_hat\n",
    "\n",
    "eps_w = 1.0\n",
    "w_hat = torch.randint(low=-1, high=1+1, size=(n,))\n",
    "w     = eps_w * w_hat\n",
    "\n",
    "mi    = -3.0\n",
    "sigma = 0.5\n",
    "gamma = 3.0\n",
    "beta  = 1.5\n",
    "\n",
    "eps_s = 3.5\n",
    "\n",
    "\n",
    "def original(x, w, mi, sigma, gamma, beta, eps_s):\n",
    "    return (((torch.dot(x, w) - mi) / sigma) * gamma + beta) / eps_s\n",
    "\n",
    "\n",
    "def fold_dotaddandmultiply(eps_x, eps_w, mi, sigma, gamma, beta, eps_s):\n",
    "    \n",
    "    gammaprime  = eps_x * eps_w * gamma / (eps_s * sigma)\n",
    "    betaprime   = (sigma * beta - mi * gamma) / (eps_x * eps_w * gamma)\n",
    "\n",
    "    return betaprime, gammaprime\n",
    "\n",
    "\n",
    "def dotaddandmultiply(x_hat, w_hat, betaprime, gammaprime):\n",
    "    return (torch.dot(x_hat, w_hat) + betaprime) * gammaprime\n",
    "    \n",
    "\n",
    "def fold_dotmultiplyandadd(eps_x, eps_w, mi, sigma, gamma, beta, eps_s):\n",
    "\n",
    "    gammasecond = eps_x * eps_w * gamma / (eps_s * sigma)\n",
    "    betasecond  = (sigma * beta - mi * gamma) / (eps_s * sigma)\n",
    "    \n",
    "    return gammasecond, betasecond\n",
    "\n",
    "\n",
    "def dotmultiplyandadd(x_hat, w_hat, gammasecond, betasecond):\n",
    "    return torch.dot(x_hat, w_hat) * gammasecond + betasecond\n",
    "\n",
    "\n",
    "\n",
    "o   = original(x, w, mi, sigma, gamma, beta, eps_s)\n",
    "fam = dotaddandmultiply(x_hat, w_hat, *fold_dotaddandmultiply(eps_x, eps_w, mi, sigma, gamma, beta, eps_s))\n",
    "fma = dotmultiplyandadd(x_hat, w_hat, *fold_dotmultiplyandadd(eps_x, eps_w, mi, sigma, gamma, beta, eps_s))\n",
    "\n",
    "print(\"Original value:                {:10.6f}\".format(o))\n",
    "print(\"Folded add-and-multiply value: {:10.6f}\".format(fam))\n",
    "print(\"Folded multiply-and-add value: {:10.6f}\".format(fma))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "282ef934",
   "metadata": {},
   "source": [
    "## Bonus section: investigating sources of numerical discrepancy\n",
    "\n",
    "As you can expect, the first time that we wrote arithmetic folding functions we made logical mistakes.\n",
    "Unfortunately, we realised it only after the code that compared the outputs of the fake-quantized and the true-quantized network output discrepancies of several tens of units.\n",
    "\n",
    "Finding the causes of such numerical discrepancies is a time-consuming process, and one of the most helpful procedures has been manually recreating the stacks of fuction calls issued by the Python interpreter when going through the `forward` method of a PyTorch network object.\n",
    "\n",
    "To repeat this stack, the prerequisite is having access to the operands involved.\n",
    "Whereas this is easy for what concerns parameters, it can be more complicated to compute the components of the `torch.Tensor` that, convolved with a filter of a convolutional module, led to a numerical discrepancy between the fake- and true-quantized versions of a network.\n",
    "\n",
    "\n",
    "\n",
    "In the following cell we implement a function that can automatically compute the sub-structure of an input `torch.Tensor` that gave rise to a discrepancy at a given position in the output `torch.Tensor` resulting from the application of a convolutional PyTorch module.\n",
    "Hopefully, this will save your time when investigating numerical discrepancies arising during your *fake2true* conversions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "83de8ede",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "from typing import Tuple\n",
    "\n",
    "\n",
    "def extract_fov_convnd(x:            torch.Tensor,\n",
    "                       convmod:      torch.nn.modules.conv._ConvNd,\n",
    "                       idx_in_batch: int,\n",
    "                       out_position: Tuple[int]):\n",
    "    \"\"\"Compute the patch (*field-of-view*, FOV) onto which the convolutional filter should be applied.\"\"\"\n",
    "\n",
    "    padding  = tuple(itertools.chain.from_iterable([(p, p) for p in convmod.padding]))\n",
    "    x_padded = torch.nn.functional.pad(x, padding)\n",
    "    \n",
    "    kernel   = convmod.kernel_size\n",
    "    stride   = convmod.stride\n",
    "    dilation = 1\n",
    "    spatial_slices = list(map(lambda t: slice(t[0] * t[1], t[0] * t[1] + t[2], dilation), zip(out_position, stride, kernel)))\n",
    "    \n",
    "    fov = x_padded[(slice(idx_in_batch, idx_in_batch + 1, 1), slice(None, None, 1)) + tuple(slice_ for slice_ in spatial_slices)]\n",
    "\n",
    "    return fov\n",
    "\n",
    "\n",
    "def decompose_diff_coords(diff_coords: Tuple[int]) -> Tuple[int, int, Tuple[int]]:\n",
    "    \n",
    "    idx_in_batch = diff_coords[0]\n",
    "    out_channel  = diff_coords[1]\n",
    "    out_position = tuple(diff_coords[2:])\n",
    "    \n",
    "    return idx_in_batch, out_channel, out_position\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa8f5d09",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
